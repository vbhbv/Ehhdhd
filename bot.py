import os
import asyncio
import tempfile
import aiofiles
import random 
import json # New: For context data
from aiohttp import ClientSession
from bs4 import BeautifulSoup
from telegram import InlineKeyboardButton, InlineKeyboardMarkup, Update
from telegram.ext import ApplicationBuilder, CommandHandler, CallbackQueryHandler, ContextTypes 
from playwright.async_api import async_playwright, Page 
from urllib.parse import urljoin 
from ddgs import DDGS 

# --- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø¨ÙˆØª ÙˆØ§Ù„Ø«ÙˆØ§Ø¨Øª ---
BOT_TOKEN = os.getenv("BOT_TOKEN")

USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
USER_AGENT_HEADER = {'User-Agent': USER_AGENT}

MIN_PDF_SIZE_BYTES = 50 * 1024 
TEMP_LINKS_KEY = "current_search_links" 
TRUSTED_DOMAINS = [
    "kotobati.com", 
    "masaha.org", 
    "books-library.net"
]

# --- Ø¯Ø§Ù„Ø© Ø§Ù„Ø¨Ø­Ø« (DDGS - Ø¨Ø¯ÙˆÙ† ØªØºÙŠÙŠØ±) ---
async def search_duckduckgo(query: str):
    sites_query = " OR ".join([f"site:{d}" for d in TRUSTED_DOMAINS])
    full_query = f"{query} filetype:pdf OR {sites_query}"
    results = []
    
    try:
        with DDGS(timeout=5) as ddgs:
            search_results = ddgs.text(full_query, max_results=10)
            for r in search_results:
                link = r.get("href")
                title = r.get("title")
                if title and link and (any(d in link for d in TRUSTED_DOMAINS) or link.lower().endswith(".pdf")):
                    is_general_section = ("kotobati.com" in link and ("/section/" in link or "/category/" in link))
                    if not is_general_section:
                         results.append({"title": title.strip(), "link": link})
    except Exception as e:
        print(f"DDGS search failed: {e}")
        return []

    unique_links = {}
    for item in results:
        unique_links[item['link']] = item
    
    return list(unique_links.values())[:5]


# --- Ø§Ù„Ø¥Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ø±Ø§Ø¨Ø¹Ø© Ø§Ù„Ù…Ø¨ØªÙƒØ±Ø©: Ø§Ù„ØªÙ†Ù‚ÙŠØ¨ ÙÙŠ Ø¬Ù…ÙŠØ¹ Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø´Ø¨ÙƒØ© (V13.0) ---
async def fallback_strategy_4_network_mine(page: Page, download_selector_css: str, link: str):
    
    network_urls = set()
    def capture_url(response):
        if response.status in [200, 206, 301, 302]:
            network_urls.add(response.url)
            
    page.on("response", capture_url)
    
    try:
        try:
             await page.locator(download_selector_css).scroll_into_view_if_needed(timeout=5000)
        except:
             pass
             
        await page.locator(download_selector_css).click(timeout=10000, force=True) 
        await asyncio.sleep(7) 
        
        for url in network_urls:
            url_lower = url.lower()
            if url_lower.endswith('.pdf') or 'drive.google.com' in url_lower or 'dropbox.com' in url_lower or 'archive.org/download' in url_lower:
                print(f"PDF link found via Network Mining: {url}")
                return url
        
        return None 
        
    except Exception as e:
        return None
        
    finally:
        try:
            page.remove_listener("response", capture_url)
        except:
            pass 

# ----------------------------------------------------------------------
# --- Ø¯Ø§Ù„Ø© Ø§Ù„Ø§Ø³ØªØ®Ù„Ø§Øµ Ø§Ù„Ù…Ø·Ù„Ù‚Ø© Ø§Ù„Ù…ÙØ·ÙˆÙ‘Ø±Ø© (V15.0 - Ø§Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù‡Ù†Ø¯Ø³ÙŠ) ---
# ----------------------------------------------------------------------
async def get_pdf_link_from_page(link: str):
    """
    ØªØ³ØªØ®Ø¯Ù… Playwright ØªØ­ØµÙŠÙ†Ø§Ù‹ Ø³Ù„ÙˆÙƒÙŠØ§Ù‹ ÙˆÙ†Ù‚Ø±Ø§Ù‹ Ù‚Ø³Ø±ÙŠØ§Ù‹ ÙˆÙ…Ù†ØµØª Ø§Ù„ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ù…Ø¨Ø§Ø´Ø± 
    Ù…Ø¹ Ø¥Ø®ÙØ§Ø¡ Ø§Ù„Ù‡ÙˆÙŠØ© Ø§Ù„Ø±Ù‚Ù…ÙŠØ© ÙˆØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ù„ØªØ¬Ø§ÙˆØ² Ø£Ù‚Ø³Ù‰ Ø­Ù…Ø§ÙŠØ© Ø§Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„Ø¨ÙˆØªØ§Øª.
    """
    pdf_link = None
    page_title = "book" 
    browser = None 
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ø£ÙˆÙ„ 
    if link.lower().endswith('.pdf') or 'archive.org/download' in link.lower() or 'drive.google.com' in link.lower():
        return link, "Direct PDF", False
        
    try:
        async with async_playwright() as p:
            # ğŸ’¥ (V15.0) Ø¥Ø·Ù„Ø§Ù‚ Ù…ØªØµÙØ­ Chrome Ø¨ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ø°Ø§ÙƒØ±Ø© ÙˆØ§Ù„Ø¥Ø®ÙØ§Ø¡ Ø§Ù„Ù‡Ù†Ø¯Ø³ÙŠ
            browser = await p.chromium.launch(
                headless="new", 
                args=[
                    '--disable-dev-shm-usage', 
                    '--no-sandbox', 
                    '--disable-setuid-sandbox',
                    '--disable-blink-features=AutomationControlled', 
                    f'--user-agent={USER_AGENT}' 
                ]
            )
            context = await browser.new_context(
                viewport={'width': 1920, 'height': 1080}, 
                user_agent=USER_AGENT,
                locale='ar-EG', # ğŸ›¡ï¸ Ø§Ù„Ø¥Ø®ÙØ§Ø¡ Ø§Ù„Ù‡Ù†Ø¯Ø³ÙŠ: Ù…Ø­Ø§ÙƒØ§Ø© Ø§Ù„Ù…ÙˆÙ‚Ø¹
            )
            
            # ğŸ›¡ï¸ Ø§Ù„Ø¥Ø®ÙØ§Ø¡ Ø§Ù„Ù‡Ù†Ø¯Ø³ÙŠ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… (Anti-Detection Script)
            await context.add_init_script("""
                // V15.0: Ø¥Ø®ÙØ§Ø¡ Ø®ØµØ§Ø¦Øµ Ù…ØªÙ‚Ø¯Ù…Ø©
                Object.defineProperty(navigator, 'webdriver', {
                    get: () => undefined
                });
                Object.defineProperty(navigator.permissions, 'query', {
                    value: (params) => (
                        params.name === 'notifications' ? Promise.resolve({ state: Notification.permission }) : query(params)
                    ),
                    enumerable: true,
                    configurable: true,
                    writable: true,
                });
                Object.defineProperty(navigator, 'platform', {
                    get: () => 'Win32'
                });
                Object.defineProperty(navigator.plugins, 'length', {
                    get: () => 3
                });
            """)
            
            page = await context.new_page()

            await page.goto(link, wait_until="domcontentloaded", timeout=40000) 
            
            # Ø§Ù„Ø§Ø¨ØªÙƒØ§Ø± Ø§Ù„Ø³Ù„ÙˆÙƒÙŠ: Ø§Ù„ØªÙ…Ø±ÙŠØ± ÙˆØ§Ù„ØªØ£Ø®ÙŠØ± Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠ 
            try:
                await page.mouse.wheel(0, random.randint(300, 800)) 
                await asyncio.sleep(random.uniform(1.5, 3))         
                await page.mouse.wheel(0, -random.randint(200, 500)) 
                await asyncio.sleep(random.uniform(1, 2.5))
            except Exception:
                 pass
            
            html_content = await page.content()
            soup = BeautifulSoup(html_content, "html.parser")
            page_title = soup.title.string if soup.title else "book"
            download_selector_css = 'a[href*="pdf"], a.book-dl-btn, a.btn-download, button:has-text("ØªØ­Ù…ÙŠÙ„"), a:has-text("Download"), a:has-text("Ø§Ø¨Ø¯Ø£ Ø§Ù„ØªØ­Ù…ÙŠÙ„"), a:has-text("Ø§Ø¶ØºØ· Ù‡Ù†Ø§ Ù„Ù„ØªØ­Ù…ÙŠÙ„")'
            
            # --- Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª (2ØŒ 1ØŒ 4ØŒ 5ØŒ 6ØŒ 7) ---
            
            # 1. Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø± Ø§Ù„Ø°ÙƒÙŠ (Strategy 2)
            try:
                await page.wait_for_selector('a[href$=".pdf"], a[href*="download"], a[href*="drive.google.com"]', timeout=10000)
                html_content = await page.content()
                soup = BeautifulSoup(html_content, "html.parser")
                for a_tag in soup.find_all('a', href=True):
                    href = urljoin(link, a_tag['href'])
                    if href.lower().endswith('.pdf') or 'download' in href.lower() or 'drive.google.com' in href.lower():
                        pdf_link = href
                        break
            except Exception:
                pass 
                
            if not pdf_link:
                
                # 2. Ø§Ù„ØªØ²Ø§Ù…Ù† (gather) (Strategy 1)
                try:
                    pdf_response, _ = await asyncio.gather(
                        page.wait_for_response(
                            lambda response: response.status in [200, 206, 301, 302] and (
                                'application/pdf' in response.headers.get('content-type', '') or 
                                response.url.lower().endswith('.pdf')
                            ),
                            timeout=30000
                        ),
                        page.locator(download_selector_css).scroll_into_view_if_needed(timeout=5000),
                        page.locator(download_selector_css).click(timeout=25000, force=True)
                    )
                    pdf_link = pdf_response.url
                    
                except Exception:
                    
                    # 3. Ø§Ù„ØªÙ†Ù‚ÙŠØ¨ Ø§Ù„Ø´Ø¨ÙƒÙŠ Ø§Ù„Ø¹Ù…ÙŠÙ‚ (Strategy 4)
                    pdf_link = await fallback_strategy_4_network_mine(page, download_selector_css, link)
            
            # 4. Ø§Ù„Ø§Ø³ØªÙ…Ø§Ø¹ Ù„Ù„Ù†Ø§ÙØ°Ø© Ø§Ù„Ù…Ù†Ø¨Ø«Ù‚Ø© (Strategy 5)
            if not pdf_link:
                try:
                    popup_event = await asyncio.gather(
                        page.wait_for_event('popup', timeout=10000), 
                        page.locator(download_selector_css).scroll_into_view_if_needed(timeout=5000),
                        page.locator(download_selector_css).click(timeout=10000, force=True) 
                    )
                    popup_page = popup_event[0]
                    await popup_page.wait_for_load_state("domcontentloaded")
                    popup_url = popup_page.url.lower()
                    if popup_url.endswith('.pdf') or 'drive.google.com' in popup_url or 'dropbox.com' in popup_url:
                        pdf_link = popup_page.url
                    await popup_page.close()
                except Exception:
                    pass
            
            # 5. Ù…ÙÙ†ØµØª Ø§Ù„ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ù‚Ø³Ø±ÙŠ Ù…Ø¹ Ø­ÙØ¸ Playwright (Strategy 6)
            is_local_path = False
            if not pdf_link:
                download_event = None
                temp_dir = tempfile.gettempdir()
                temp_file_name = f"temp_{os.getpid()}_{random.randint(100, 999)}.pdf"
                temp_file_path = os.path.join(temp_dir, temp_file_name)

                def capture_download(download):
                    nonlocal download_event
                    download_event = download

                page.on('download', capture_download)

                try:
                    # ğŸ’¥ V15.0: Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ù†Ù‚Ø± Ø§Ù„Ù…Ø²Ø¯ÙˆØ¬ Ù„Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ù…ÙˆØ«ÙˆÙ‚ÙŠØ©
                    await page.locator(download_selector_css).scroll_into_view_if_needed(timeout=5000)
                    await page.locator(download_selector_css).click(timeout=15000, force=True)
                    await asyncio.sleep(5) 
                    # Ø§Ù„Ù†Ù‚Ø± Ø§Ù„Ø«Ø§Ù†ÙŠ
                    if not download_event:
                        await page.locator(download_selector_css).click(timeout=10000, force=True)
                        await asyncio.sleep(5) 

                    if download_event:
                        await download_event.save_as(temp_file_path)
                        pdf_link = temp_file_path  # Ù†Ø±Ø¬Ø¹ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ø­Ù„ÙŠ
                        is_local_path = True
                        
                except Exception as e:
                    pdf_link = None 

                finally:
                    try:
                        page.remove_listener('download', capture_download)
                    except:
                        pass
            
            # 6. Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ù†Ù‚Ø± Ø§Ù„Ù‚Ø³Ø±ÙŠ Ø¨Ù€ JavaScript (Strategy 7)
            if not pdf_link:
                 try:
                    await page.evaluate(f"""
                        const element = document.querySelector('{download_selector_css.replace("'", "\\'")}');
                        if (element) {{
                            element.click();
                        }}
                    """)
                    await asyncio.sleep(5) 

                 except Exception as e:
                    pass
            
            # 7. ÙØ­Øµ HTML Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ (Strategy 3)
            if not pdf_link:
                await asyncio.sleep(5) 
                final_html_content = await page.content()
                final_soup = BeautifulSoup(final_html_content, "html.parser")
                for a_tag in final_soup.find_all('a', href=True):
                    href = urljoin(link, a_tag['href'])
                    href_lower = href.lower()
                    if href_lower.endswith('.pdf') or 'download' in href_lower:
                        pdf_link = href
                        break

            # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
            if not page_title:
                 html_content = await page.content()
                 soup = BeautifulSoup(html_content, "html.parser")
                 page_title = soup.title.string if soup.title else "book"

            return pdf_link, page_title, is_local_path 
    
    except Exception as e:
        return None, "book", False
    
    finally:
        if browser:
            await browser.close()


# ----------------------------------------------------------------------
# --- Ø¯Ø§Ù„Ø© Ø§Ù„ØªØ­Ù…ÙŠÙ„ ÙˆØ§Ù„Ø¥Ø±Ø³Ø§Ù„ (V15.0 - Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ù…ÙˆØ«ÙˆÙ‚) ---
# ----------------------------------------------------------------------
async def download_and_send_pdf(context, chat_id, source, title="book.pdf", is_local_path=False):
    """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„ÙØŒ Ø¥Ø±Ø³Ø§Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ØŒ Ø«Ù… Ø­Ø°ÙÙ‡ Ù…Ù† Ø§Ù„Ù‚Ø±Øµ Ø§Ù„ØµÙ„Ø¨."""
    
    if is_local_path:
        file_path = source 
    else:
        pdf_url = source
        
        async with ClientSession() as session:
            # âœ… V15.0: Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ù…ÙˆØ«ÙˆÙ‚ Ù…Ù† Ø§Ù„Ø±Ø£Ø³ Ù‚Ø¨Ù„ Ø§Ù„ØªØ­Ù…ÙŠÙ„
            try:
                async with session.head(pdf_url, headers=USER_AGENT_HEADER, allow_redirects=True, timeout=10) as head_resp:
                    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù† Ø§Ù„Ø±Ø§Ø¨Ø· Ù„ÙŠØ³ ØµÙØ­Ø© HTML
                    content_type = head_resp.headers.get('Content-Type', '').lower()
                    content_length = int(head_resp.headers.get('Content-Length', 0))

                    if 'application/pdf' not in content_type and 'octet-stream' not in content_type:
                        await context.bot.send_message(chat_id=chat_id, text=f"âš ï¸ ÙØ´Ù„ Ø§Ù„ØªØ­Ù…ÙŠÙ„: Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„Ù…Ø³ØªØ®Ù„Øµ Ù„Ø§ ÙŠØ´ÙŠØ± Ø¥Ù„Ù‰ Ù…Ù„Ù PDF ({content_type}).")
                        return

                    if content_length < MIN_PDF_SIZE_BYTES:
                         await context.bot.send_message(chat_id=chat_id, text="âš ï¸ ÙØ´Ù„ Ø§Ù„ØªØ­Ù…ÙŠÙ„: Ø­Ø¬Ù… Ø§Ù„Ù…Ù„Ù ØµØºÙŠØ± Ø¬Ø¯Ø§Ù‹ (ØºÙŠØ± ØµØ§Ù„Ø­).")
                         return
            except Exception as e:
                await context.bot.send_message(chat_id=chat_id, text=f"âš ï¸ ÙØ´Ù„ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø±Ø£Ø³ Ø§Ù„Ù…Ù„Ù (HEAD Check): {e}")
                return
            
            # Ø§Ù„Ù…ØªØ§Ø¨Ø¹Ø© Ø¨Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙØ¹Ù„ÙŠØ©
            tmp_dir = tempfile.gettempdir()
            file_path = os.path.join(tmp_dir, title.replace("/", "_")[:40] + ".pdf")

            async with session.get(pdf_url, headers=USER_AGENT_HEADER) as resp:
                if resp.status != 200:
                    await context.bot.send_message(chat_id=chat_id, text=f"âš ï¸ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù Ù…Ù† Ø§Ù„Ù…ØµØ¯Ø±. Ø±Ù…Ø² Ø§Ù„Ø®Ø·Ø£: {resp.status}")
                    return
                
                content = await resp.read()
                
                async with aiofiles.open(file_path, "wb") as f:
                    await f.write(content)

    # --- Ù…Ù†Ø·Ù‚ Ø§Ù„Ø¥Ø±Ø³Ø§Ù„ ÙˆØ§Ù„ØªÙ†Ø¸ÙŠÙ ---
    try:
        with open(file_path, "rb") as f:
            await context.bot.send_document(chat_id=chat_id, document=f)
        await context.bot.send_message(chat_id=chat_id, text="âœ… ØªÙ… Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„ÙƒØªØ§Ø¨ Ø¨Ù†Ø¬Ø§Ø­.")
    except Exception as e:
         await context.bot.send_message(chat_id=chat_id, text=f"âš ï¸ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù…Ù„Ù Ø¥Ù„Ù‰ ØªÙŠÙ„ÙŠØ¬Ø±Ø§Ù…: {e}")
    finally:
        if os.path.exists(file_path):
            os.remove(file_path)

# --- Ø¯Ø§Ù„Ø© Callback (Ù…Ø¹ ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù†Øµ) ---
async def callback_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    query = update.callback_query
    await query.answer()
    data = query.data
    
    if data.startswith("dl|"):
        try:
            index_str = data.split("|", 1)[1]
            index = int(index_str)
            link = context.user_data[TEMP_LINKS_KEY][index]

        except Exception:
            await context.bot.send_message(chat_id=query.message.chat_id, text="âš ï¸ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø²Ø± Ø§Ù„ØªØ­Ù…ÙŠÙ„ (Ø±Ø§Ø¨Ø· ØºÙŠØ± ØµØ§Ù„Ø­).")
            return
            
        await query.edit_message_text("â³ ØªÙØ¹ÙŠÙ„ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ø§Ø³ØªØ®Ù„Ø§Øµ Ø§Ù„Ù†Ø§Ø±ÙŠ (V15.0 - Ø§Ù„Ù…Ù‚Ø§ÙˆÙ…Ø© Ø§Ù„Ù‡Ù†Ø¯Ø³ÙŠØ©)...")
        
        try:
            pdf_link, title, is_local_path = await get_pdf_link_from_page(link)
            
            if pdf_link:
                await download_and_send_pdf(context, query.message.chat_id, pdf_link, title=title if title else "book", is_local_path=is_local_path)
            else:
                await context.bot.send_message(chat_id=query.message.chat_id, text=f"ğŸ“„ ÙØ´Ù„ Ø§Ù„Ø§Ø³ØªØ®Ù„Ø§Øµ. Ø±Ø§Ø¨Ø· Ø§Ù„Ù…ØµØ¯Ø±: {link}")
        
        except Exception as e:
            await context.bot.send_message(chat_id=query.message.chat_id, text=f"âš ï¸ Ø®Ø·Ø£ Playwright Ø£Ø«Ù†Ø§Ø¡ Ø¬Ù„Ø¨ Ø§Ù„Ù…Ù„Ù: {e}")

# --- Ø¨Ø§Ù‚ÙŠ Ø¯ÙˆØ§Ù„ ØªÙŠÙ„ÙŠØ¬Ø±Ø§Ù… (startØŒ search_cmdØŒ main) ---

async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text(
        "ğŸ“š Ø¨ÙˆØª Ø§Ù„Ù‚ÙŠØ§Ù…Ø© Ø¬Ø§Ù‡Ø²!\n"
        "Ø£Ø±Ø³Ù„ /search Ù…ØªØ¨ÙˆØ¹Ù‹Ø§ Ø¨Ø§Ø³Ù… Ø§Ù„ÙƒØªØ§Ø¨ Ø£Ùˆ Ø§Ù„Ù…Ø¤Ù„Ù."
    )

async def search_cmd(update: Update, context: ContextTypes.DEFAULT_TYPE):
    query = " ".join(context.args).strip()
    if not query:
        await update.message.reply_text("Ø§Ø³ØªØ®Ø¯Ù…: /search Ø§Ø³Ù… Ø§Ù„ÙƒØªØ§Ø¨ Ø£Ùˆ Ø§Ù„Ù…Ø¤Ù„Ù")
        return

    msg = await update.message.reply_text(f"ğŸ” Ø£Ø¨Ø­Ø« Ø¹Ù† **{query}** Ø¹Ø¨Ø± **DuckDuckGo** (ÙÙ„ØªØ±Ø© ØµØ§Ø±Ù…Ø© Ù„Ù„Ù†ØªØ§Ø¦Ø¬)...")
    
    try:
        results = await search_duckduckgo(query)

        if not results:
            await msg.edit_text("âŒ Ù„Ù… Ø£Ø¬Ø¯ Ù†ØªØ§Ø¦Ø¬ Ù…ÙˆØ«ÙˆÙ‚Ø© ÙÙŠ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù…Ø®ØªØ§Ø±Ø©. Ø­Ø§ÙˆÙ„ Ø¨ÙƒÙ„Ù…Ø§Øª Ù…Ø®ØªÙ„ÙØ© Ø£Ùˆ Ø¬Ø±Ø¨ Ø§Ù„Ø¨Ø­Ø« Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.")
            return

        buttons = []
        text_lines = []
        
        context.user_data[TEMP_LINKS_KEY] = [item.get("link") for item in results]
        
        for i, item in enumerate(results, start=0):
            title = item.get("title")[:120]
            source = next((d.replace('.com', '').replace('.net', '') for d in TRUSTED_DOMAINS if d in item.get('link')), "Ù…Ø¨Ø§Ø´Ø±/Ø¹Ø§Ù…")
            text_lines.append(f"{i+1}. {title} (Ø§Ù„Ù…ØµØ¯Ø±: {source})")
            buttons.append([InlineKeyboardButton(f"ğŸ“¥ ØªØ­Ù…ÙŠÙ„ {i+1}", callback_data=f"dl|{i}")])
            
        reply = "\n".join(text_lines)
        await msg.edit_text(reply, parse_mode='Markdown', reply_markup=InlineKeyboardMarkup(buttons))
        
    except Exception as e:
         await msg.edit_text(f"âš ï¸ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø¨Ø­Ø«: {e}")

def main():
    if not BOT_TOKEN:
        raise ValueError("BOT_TOKEN is missing in environment variables.")

    app = ApplicationBuilder().token(BOT_TOKEN).build()

    app.add_handler(CommandHandler("start", start))
    app.add_handler(CommandHandler("search", search_cmd))
    app.add_handler(CallbackQueryHandler(callback_handler))

    print("Ø§Ù„Ø¨ÙˆØª Ø¨Ø¯Ø£ Ø§Ù„Ø¹Ù…Ù„.")
    app.run_polling()

if __name__ == "__main__":
    main()
