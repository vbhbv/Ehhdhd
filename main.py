import os
import asyncio
import tempfile
import aiofiles
import random 
from aiohttp import ClientSession
from bs4 import BeautifulSoup
from telegram import InlineKeyboardButton, InlineKeyboardMarkup, Update
from telegram.ext import ApplicationBuilder, CommandHandler, CallbackQueryHandler, ContextTypes 
from playwright.async_api import async_playwright, Page 
from urllib.parse import urljoin 
from ddgs import DDGS 

# --- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø¨ÙˆØª ÙˆØ§Ù„Ø«ÙˆØ§Ø¨Øª ---
BOT_TOKEN = os.getenv("BOT_TOKEN")

# ÙˆÙƒÙŠÙ„ Ù…Ø³ØªØ®Ø¯Ù… Ù„Ø³Ø·Ø­ Ø§Ù„Ù…ÙƒØªØ¨ (Ù„Ù„ØªØ­ØµÙŠÙ† Ø¶Ø¯ Ø§Ù„ÙƒØ´Ù)
USER_AGENT = 'Mozilla/50 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
USER_AGENT_HEADER = {'User-Agent': USER_AGENT}

MIN_PDF_SIZE_BYTES = 50 * 1024 
TEMP_LINKS_KEY = "current_search_links" 
TRUSTED_DOMAINS = [
    "kotobati.com", 
    "masaha.org", 
    "books-library.net"
]

# --- Ø¯Ø§Ù„Ø© Ø§Ù„Ø¨Ø­Ø« (DDGS - Ø¨Ø¯ÙˆÙ† ØªØºÙŠÙŠØ±) ---
async def search_duckduckgo(query: str):
    sites_query = " OR ".join([f"site:{d}" for d in TRUSTED_DOMAINS])
    full_query = f"{query} filetype:pdf OR {sites_query}"
    results = []
    
    try:
        with DDGS(timeout=5) as ddgs:
            search_results = ddgs.text(full_query, max_results=10)
            for r in search_results:
                link = r.get("href")
                title = r.get("title")
                if title and link and (any(d in link for d in TRUSTED_DOMAINS) or link.lower().endswith(".pdf")):
                    is_general_section = ("kotobati.com" in link and ("/section/" in link or "/category/" in link))
                    if not is_general_section:
                         results.append({"title": title.strip(), "link": link})
    except Exception as e:
        print(f"DDGS search failed: {e}")
        return []

    unique_links = {}
    for item in results:
        unique_links[item['link']] = item
    
    return list(unique_links.values())[:5]

# ----------------------------------------------------------------------
# --- Ø¯Ø§Ù„Ø© Ø§Ù„Ø§Ø³ØªØ®Ù„Ø§Øµ Ø§Ù„Ù…Ø·Ù„Ù‚Ø© Ø§Ù„Ù…ÙØ­Ø³Ù‘ÙÙ†Ø© (V12.1.2 - Ø§Ù„Ø¶Ø±Ø¨Ø© Ø§Ù„ØªÙƒØªÙŠÙƒÙŠØ© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©) ---
# ----------------------------------------------------------------------
async def get_pdf_link_from_page(link: str):
    """
    Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù„ØªØ¹Ø¯ÙŠÙ†: Ø¥Ø®ÙØ§Ø¡ Ø§Ù„Ù‡ÙˆÙŠØ©ØŒ Ù…Ø­Ø§ÙƒØ§Ø© Ø§Ù„Ø³Ù„ÙˆÙƒØŒ Ø§Ù„Ù†Ù‚Ø± Ø§Ù„Ù‚Ø³Ø±ÙŠØŒ ÙˆØ­Ù„ Blob Ø§Ù„Ø¬Ø°Ø±ÙŠ.
    """
    pdf_link = None
    page_title = "book" 
    browser = None 
    
    # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ù„Ø¶Ù…Ø§Ù† Ø¹Ø¯Ù… Ø­Ø¯ÙˆØ« UnboundLocalError
    is_local_path = False 
    network_urls = set() 
    
    if link.lower().endswith('.pdf') or 'archive.org/download' in link.lower() or 'drive.google.com' in link.lower():
        return link, "Direct PDF", False
        
    try:
        async with async_playwright() as p:
            browser = await p.chromium.launch(
                headless=True,
                args=[
                    '--no-sandbox', 
                    '--disable-setuid-sandbox',
                    '--disable-blink-features=AutomationControlled', 
                    f'--user-agent={USER_AGENT}' 
                ]
            )
            context = await browser.new_context(
                viewport={'width': 1920, 'height': 1080},
                user_agent=USER_AGENT
            )
            
            # ğŸ’¥ Ø¥Ø®ÙØ§Ø¡ Ø§Ù„Ù‡ÙˆÙŠØ© Ø§Ù„Ø±Ù‚Ù…ÙŠØ© (Anti-Detection Script)
            await context.add_init_script("""
                Object.defineProperty(navigator, 'webdriver', {
                    get: () => undefined
                });
            """)
            
            page = await context.new_page()

            await page.goto(link, wait_until="domcontentloaded", timeout=40000) 
            
            # ğŸ’¥ Ù…Ø­Ø§ÙƒØ§Ø© Ø§Ù„Ø³Ù„ÙˆÙƒ (Behavioral Emulation)
            try:
                await page.mouse.wheel(0, random.randint(300, 800)) 
                await asyncio.sleep(random.uniform(1.5, 3))         
                await page.mouse.wheel(0, -random.randint(200, 500)) 
                await asyncio.sleep(random.uniform(1, 2.5))
            except Exception:
                 pass
            
            html_content = await page.content()
            soup = BeautifulSoup(html_content, "html.parser")
            page_title = soup.title.string if soup.title else "book"
            download_selector_css = 'a[href*="pdf"], a.book-dl-btn, a.btn-download, button:has-text("ØªØ­Ù…ÙŠÙ„"), a:has-text("Download"), a:has-text("Ø§Ø¨Ø¯Ø£ Ø§Ù„ØªØ­Ù…ÙŠÙ„"), a:has-text("Ø§Ø¶ØºØ· Ù‡Ù†Ø§ Ù„Ù„ØªØ­Ù…ÙŠÙ„")'
            
            # 1. Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø± Ø§Ù„Ø°ÙƒÙŠ (Strategy 2)
            try:
                await page.wait_for_selector('a[href$=".pdf"], a[href*="download"], a[href*="drive.google.com"]', timeout=10000)
                html_content = await page.content()
                soup = BeautifulSoup(html_content, "html.parser")
                for a_tag in soup.find_all('a', href=True):
                    href = urljoin(link, a_tag['href'])
                    if href.lower().endswith('.pdf') or 'download' in href.lower() or 'drive.google.com' in href.lower():
                        pdf_link = href
                        break
            except Exception:
                pass 
                
            # --- Ø¥Ø°Ø§ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ø±Ø§Ø¨Ø·ØŒ Ù†Ø¨Ø¯Ø£ Ø¯ÙˆØ±Ø© Ø§Ù„Ù†Ù‚Ø± ÙˆØ§Ù„Ù…Ù†ØµØªØ§Øª ---
            if not pdf_link:
                
                # Ø¥Ø¹Ø¯Ø§Ø¯ Ù…Ù†ØµØª Ø§Ù„Ø´Ø¨ÙƒØ©
                def capture_url(response):
                    if response.status in [200, 206, 301, 302]:
                        network_urls.add(response.url)
                page.on("response", capture_url)
                
                # Ø¥Ø¹Ø¯Ø§Ø¯ Ù…Ù†ØµØª Ø§Ù„ØªÙ†Ø²ÙŠÙ„ (Download Listener)
                download_event = None
                temp_dir = tempfile.gettempdir()
                temp_file_name = f"temp_{os.getpid()}_{random.randint(100, 999)}.pdf"
                temp_file_path = os.path.join(temp_dir, temp_file_name)
                
                def capture_download(download):
                    nonlocal download_event
                    download_event = download
                page.on('download', capture_download)

                # --- ğŸ’¥ Ø§Ù„Ø¶Ø±Ø¨Ø© Ø§Ù„ØªÙƒØªÙŠÙƒÙŠØ©: Ø§Ù„Ù†Ù‚Ø± Ø§Ù„Ù‚Ø³Ø±ÙŠ Ø§Ù„Ø£ÙˆÙ„ ÙˆØ§Ù„ÙˆØ­ÙŠØ¯ ---
                try:
                    # Ø¬Ø¹Ù„ Ø§Ù„Ø²Ø± Ù…Ø±Ø¦ÙŠØ§Ù‹ ÙˆØ§Ù„Ù†Ù‚Ø± Ø¹Ù„ÙŠÙ‡ Ù‚Ø³Ø±ÙŠØ§Ù‹ Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø©
                    await page.locator(download_selector_css).scroll_into_view_if_needed(timeout=5000)
                    await page.locator(download_selector_css).click(timeout=15000, force=True)
                    await asyncio.sleep(7) 

                except Exception:
                     # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ù†Ù‚Ø± Ø¨Ù€ JavaScript Ø¥Ø°Ø§ ÙØ´Ù„ Ø§Ù„Ù†Ù‚Ø± Ø¨Ù€ Locator
                     try:
                        # ğŸ’¥ ØªØµØ­ÙŠØ­ V12.1.2: Ø§Ø³ØªØ®Ø¯Ø§Ù… arg Ù„ØªÙ…Ø±ÙŠØ± Ø§Ù„Ù…ØªØºÙŠØ± (Ù„Ù…Ù†Ø¹ Ø®Ø·Ø£ Ø§Ù„Ù€ f-string)
                        await page.evaluate("""
                            (selector) => {
                                const element = document.querySelector(selector);
                                if (element) {
                                    element.click();
                                }
                            }
                        """, download_selector_css) # ØªÙ…Ø±ÙŠØ± Ø§Ù„Ù…ØªØºÙŠØ± Ù‡Ù†Ø§ ÙƒÙ€ arg
                        await asyncio.sleep(7) 
                     except Exception:
                         pass
                
                # --- ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¨Ø¹Ø¯ Ø§Ù„Ù†Ù‚Ø± Ø§Ù„ÙˆØ­ÙŠØ¯ ---
                
                # 2. ØªÙ‚ÙŠÙŠÙ… Ù…Ù†ØµØª Ø§Ù„ØªÙ†Ø²ÙŠÙ„ (Strategy 6 - Blob/Local Save)
                if download_event:
                    await download_event.save_as(temp_file_path)
                    pdf_link = temp_file_path
                    is_local_path = True
                
                # 3. ØªÙ‚ÙŠÙŠÙ… Ù…Ù†ØµØª Ø§Ù„Ø´Ø¨ÙƒØ© (Strategy 4 - Network Mine)
                if not pdf_link:
                    for url in network_urls:
                        url_lower = url.lower()
                        if url_lower.endswith('.pdf') or 'drive.google.com' in url_lower or 'dropbox.com' in url_lower or 'archive.org/download' in url_lower:
                            pdf_link = url
                            break
                
                # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ù†ØµØªØ§Øª
                try:
                    page.remove_listener("response", capture_url)
                    page.remove_listener('download', capture_download)
                except:
                    pass

            # 5. ÙØ­Øµ HTML Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ (Strategy 3)
            if not pdf_link:
                await asyncio.sleep(5) 
                final_html_content = await page.content()
                final_soup = BeautifulSoup(final_html_content, "html.parser")
                for a_tag in final_soup.find_all('a', href=True):
                    href = urljoin(link, a_tag['href'])
                    href_lower = href.lower()
                    if href_lower.endswith('.pdf') or 'download' in href_lower:
                        pdf_link = href
                        break

            # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
            if not page_title:
                 html_content = await page.content()
                 soup = BeautifulSoup(html_content, "html.parser")
                 page_title = soup.title.string if soup.title else "book"

            return pdf_link, page_title, is_local_path 
    
    except Exception as e:
        return None, "book", False
    
    finally:
        if browser:
            await browser.close()


# --- Ø¯Ø§Ù„Ø© Ø§Ù„ØªØ­Ù…ÙŠÙ„ ÙˆØ§Ù„Ø¥Ø±Ø³Ø§Ù„ (Ø¨Ø¯ÙˆÙ† ØªØºÙŠÙŠØ±) ---
async def download_and_send_pdf(context, chat_id, source, title="book.pdf", is_local_path=False):
    """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„ÙØŒ Ø¥Ø±Ø³Ø§Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ØŒ Ø«Ù… Ø­Ø°ÙÙ‡ Ù…Ù† Ø§Ù„Ù‚Ø±Øµ Ø§Ù„ØµÙ„Ø¨."""
    
    if is_local_path:
        file_path = source 
    else:
        pdf_url = source
        tmp_dir = tempfile.gettempdir()
        file_path = os.path.join(tmp_dir, title.replace("/", "_")[:40] + ".pdf")
        
        async with ClientSession() as session:
            async with session.get(pdf_url, headers=USER_AGENT_HEADER) as resp:
                if resp.status != 200:
                    await context.bot.send_message(chat_id=chat_id, text=f"âš ï¸ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù Ù…Ù† Ø§Ù„Ù…ØµØ¯Ø±. Ø±Ù…Ø² Ø§Ù„Ø®Ø·Ø£: {resp.status}")
                    return
                content = await resp.read()
                if len(content) < MIN_PDF_SIZE_BYTES:
                    await context.bot.send_message(chat_id=chat_id, text="âš ï¸ ÙØ´Ù„ Ø§Ù„ØªØ­Ù…ÙŠÙ„: Ø­Ø¬Ù… Ø§Ù„Ù…Ù„Ù ØµØºÙŠØ± Ø¬Ø¯Ø§Ù‹ (ØºÙŠØ± ØµØ§Ù„Ø­).")
                    return
                async with aiofiles.open(file_path, "wb") as f:
                    await f.write(content)

    # --- Ù…Ù†Ø·Ù‚ Ø§Ù„Ø¥Ø±Ø³Ø§Ù„ ÙˆØ§Ù„ØªÙ†Ø¸ÙŠÙ ---
    try:
        with open(file_path, "rb") as f:
            await context.bot.send_document(chat_id=chat_id, document=f)
        await context.bot.send_message(chat_id=chat_id, text="âœ… ØªÙ… Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„ÙƒØªØ§Ø¨ Ø¨Ù†Ø¬Ø§Ø­.")
    except Exception as e:
         await context.bot.send_message(chat_id=chat_id, text=f"âš ï¸ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù…Ù„Ù Ø¥Ù„Ù‰ ØªÙŠÙ„ÙŠØ¬Ø±Ø§Ù…: {e}")
    finally:
        if os.path.exists(file_path):
            os.remove(file_path)

# --- Ø¯Ø§Ù„Ø© Callback (Ø¨Ø¯ÙˆÙ† ØªØºÙŠÙŠØ±) ---
async def callback_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    query = update.callback_query
    await query.answer()
    data = query.data
    
    if data.startswith("dl|"):
        try:
            index_str = data.split("|", 1)[1]
            index = int(index_str)
            link = context.user_data[TEMP_LINKS_KEY][index]

        except Exception:
            await context.bot.send_message(chat_id=query.message.chat_id, text="âš ï¸ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø²Ø± Ø§Ù„ØªØ­Ù…ÙŠÙ„ (Ø±Ø§Ø¨Ø· ØºÙŠØ± ØµØ§Ù„Ø­).")
            return
            
        await query.edit_message_text("â³ ØªÙØ¹ÙŠÙ„ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ø§Ø³ØªØ®Ù„Ø§Øµ Ø§Ù„Ù†Ø§Ø±ÙŠ (V12.1.2 - Ø§Ù„Ø¶Ø±Ø¨Ø© Ø§Ù„ØªÙƒØªÙŠÙƒÙŠØ©)...")
        
        try:
            pdf_link, title, is_local_path = await get_pdf_link_from_page(link)
            
            if pdf_link:
                await download_and_send_pdf(context, query.message.chat_id, pdf_link, title=title if title else "book", is_local_path=is_local_path)
            else:
                await context.bot.send_message(chat_id=query.message.chat_id, text=f"ğŸ“„ ÙØ´Ù„ Ø§Ù„Ø§Ø³ØªØ®Ù„Ø§Øµ. Ø±Ø§Ø¨Ø· Ø§Ù„Ù…ØµØ¯Ø±: {link}")
        
        except Exception as e:
            await context.bot.send_message(chat_id=query.message.chat_id, text=f"âš ï¸ Ø®Ø·Ø£ Playwright Ø£Ø«Ù†Ø§Ø¡ Ø¬Ù„Ø¨ Ø§Ù„Ù…Ù„Ù: {e}")

# --- Ø¨Ø§Ù‚ÙŠ Ø¯ÙˆØ§Ù„ ØªÙŠÙ„ÙŠØ¬Ø±Ø§Ù… (startØŒ search_cmdØŒ main) ---

async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text(
        "ğŸ“š Ø¨ÙˆØª Ø§Ù„Ù‚ÙŠØ§Ù…Ø© Ø¬Ø§Ù‡Ø²!\n"
        "Ø£Ø±Ø³Ù„ /search Ù…ØªØ¨ÙˆØ¹Ù‹Ø§ Ø¨Ø§Ø³Ù… Ø§Ù„ÙƒØªØ§Ø¨ Ø£Ùˆ Ø§Ù„Ù…Ø¤Ù„Ù."
    )

async def search_cmd(update: Update, context: ContextTypes.DEFAULT_TYPE):
    query = " ".join(context.args).strip()
    if not query:
        await update.message.reply_text("Ø§Ø³ØªØ®Ø¯Ù…: /search Ø§Ø³Ù… Ø§Ù„ÙƒØªØ§Ø¨ Ø£Ùˆ Ø§Ù„Ù…Ø¤Ù„Ù")
        return

    msg = await update.message.reply_text(f"ğŸ” Ø£Ø¨Ø­Ø« Ø¹Ù† **{query}** Ø¹Ø¨Ø± **DuckDuckGo** (ÙÙ„ØªØ±Ø© ØµØ§Ø±Ù…Ø© Ù„Ù„Ù†ØªØ§Ø¦Ø¬)...")
    
    try:
        results = await search_duckduckgo(query)

        if not results:
            await msg.edit_text("âŒ Ù„Ù… Ø£Ø¬Ø¯ Ù†ØªØ§Ø¦Ø¬ Ù…ÙˆØ«ÙˆÙ‚Ø© ÙÙŠ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù…Ø®ØªØ§Ø±Ø©. Ø­Ø§ÙˆÙ„ Ø¨ÙƒÙ„Ù…Ø§Øª Ù…Ø®ØªÙ„ÙØ© Ø£Ùˆ Ø¬Ø±Ø¨ Ø§Ù„Ø¨Ø­Ø« Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.")
            return

        buttons = []
        text_lines = []
        
        context.user_data[TEMP_LINKS_KEY] = [item.get("link") for item in results]
        
        for i, item in enumerate(results, start=0):
            title = item.get("title")[:120]
            source = next((d.replace('.com', '').replace('.net', '') for d in TRUSTED_DOMAINS if d in item.get('link')), "Ù…Ø¨Ø§Ø´Ø±/Ø¹Ø§Ù…")
            text_lines.append(f"{i+1}. {title} (Ø§Ù„Ù…ØµØ¯Ø±: {source})")
            buttons.append([InlineKeyboardButton(f"ğŸ“¥ ØªØ­Ù…ÙŠÙ„ {i+1}", callback_data=f"dl|{i}")])
            
        reply = "\n".join(text_lines)
        await msg.edit_text(reply, parse_mode='Markdown', reply_markup=InlineKeyboardMarkup(buttons))
        
    except Exception as e:
         await msg.edit_text(f"âš ï¸ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø¨Ø­Ø«: {e}")

def main():
    if not BOT_TOKEN:
        raise ValueError("BOT_TOKEN is missing in environment variables.")

    app = ApplicationBuilder().token(BOT_TOKEN).build()

    app.add_handler(CommandHandler("start", start))
    app.add_handler(CommandHandler("search", search_cmd))
    app.add_handler(CallbackQueryHandler(callback_handler))

    print("Ø§Ù„Ø¨ÙˆØª Ø¨Ø¯Ø£ Ø§Ù„Ø¹Ù…Ù„.")
    app.run_polling()

if __name__ == "__main__":
    main()
