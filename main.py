import os
import asyncio
import tempfile
import aiofiles
from aiohttp import ClientSession
from bs4 import BeautifulSoup
from telegram import InlineKeyboardButton, InlineKeyboardMarkup, Update
from telegram.ext import ApplicationBuilder, CommandHandler, CallbackQueryHandler, ContextTypes 
from playwright.async_api import async_playwright, Page 
from urllib.parse import urljoin 
from ddgs import DDGS 

# --- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø¨ÙˆØª ÙˆØ§Ù„Ø«ÙˆØ§Ø¨Øª ---
BOT_TOKEN = os.getenv("BOT_TOKEN")

# ÙˆÙƒÙŠÙ„ Ù…Ø³ØªØ®Ø¯Ù… Ù…Ø­Ù…ÙˆÙ„ (Mobile User Agent) Ù„Ø²ÙŠØ§Ø¯Ø© Ø§Ù„ØªØ­ØµÙŠÙ†
USER_AGENT = 'Mozilla/5.0 (iPhone; CPU iPhone OS 15_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.0 Mobile/15E148 Safari/604.1'
USER_AGENT_HEADER = {'User-Agent': USER_AGENT}

MIN_PDF_SIZE_BYTES = 50 * 1024 
TEMP_LINKS_KEY = "current_search_links" 
TRUSTED_DOMAINS = [
    "kotobati.com", 
    "masaha.org", 
    "books-library.net"
]

# --- Ø¯Ø§Ù„Ø© Ø§Ù„Ø¨Ø­Ø« (DuckDuckGo - V10.2: ÙÙ„ØªØ±Ø© Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø¹Ø§Ù…Ø©) ---
async def search_duckduckgo(query: str):
    """
    ÙŠØ³ØªØ®Ø¯Ù… DuckDuckGo API Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø±ÙˆØ§Ø¨Ø· PDF Ù…Ø¨Ø§Ø´Ø±Ø© ÙÙŠ Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„Ù…ÙˆØ«ÙˆÙ‚Ø©ØŒ 
    Ù…Ø¹ ØªØ·Ø¨ÙŠÙ‚ ÙÙ„ØªØ± ØµØ§Ø±Ù… Ù„Ø§Ø³ØªØ¨Ø¹Ø§Ø¯ Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø£Ù‚Ø³Ø§Ù… Ø§Ù„Ø¹Ø§Ù…Ø©.
    """
    
    sites_query = " OR ".join([f"site:{d}" for d in TRUSTED_DOMAINS])
    full_query = f"{query} filetype:pdf OR {sites_query}"
    
    print(f"Executing search query: {full_query}")
    
    results = []
    
    try:
        with DDGS(timeout=5) as ddgs:
            search_results = ddgs.text(full_query, max_results=10)
            
            for r in search_results:
                link = r.get("href")
                title = r.get("title")
                
                if title and link and (any(d in link for d in TRUSTED_DOMAINS) or link.lower().endswith(".pdf")):
                    
                    # ğŸ’¥ Ø§Ù„ÙÙ„ØªØ± Ø§Ù„Ø¬Ø¯ÙŠØ¯: Ø§Ø³ØªØ¨Ø¹Ø§Ø¯ Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø£Ù‚Ø³Ø§Ù… Ø§Ù„Ø¹Ø§Ù…Ø©
                    is_general_section = ("kotobati.com" in link and ("/section/" in link or "/category/" in link))
                    
                    if not is_general_section:
                         results.append({"title": title.strip(), "link": link})

    except Exception as e:
        print(f"DDGS search failed: {e}")
        return []

    unique_links = {}
    for item in results:
        unique_links[item['link']] = item
    
    return list(unique_links.values())[:5]


# --- Ø§Ù„Ø¥Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ø±Ø§Ø¨Ø¹Ø© Ø§Ù„Ù…Ø¨ØªÙƒØ±Ø©: Ø§Ù„ØªÙ†Ù‚ÙŠØ¨ ÙÙŠ Ø¬Ù…ÙŠØ¹ Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø´Ø¨ÙƒØ© ---
async def fallback_strategy_4_network_mine(page: Page, download_selector_css: str, link: str):
    
    network_urls = set()

    def capture_url(response):
        if response.status in [200, 206, 301, 302]:
            network_urls.add(response.url)
            
    page.on("response", capture_url)
    
    try:
        # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ù†Ù‚Ø± Ø£ÙˆÙ„Ø§Ù‹ Ù„ØªÙØ¹ÙŠÙ„ Ø´Ø¨ÙƒØ© Ø§Ù„ØªØ­Ù…ÙŠÙ„
        await page.locator(download_selector_css).click(timeout=10000) 
        await asyncio.sleep(7) 
        
        for url in network_urls:
            url_lower = url.lower()
            if url_lower.endswith('.pdf') or 'drive.google.com' in url_lower or 'dropbox.com' in url_lower or 'archive.org/download' in url_lower:
                print(f"PDF link found via Network Mining: {url}")
                return url
        
        return None 
        
    except Exception as e:
        print(f"Network mining click failed: {e}")
        return None
        
    finally:
        try:
            page.remove_listener("response", capture_url)
        except:
            pass 

# --- Ø¯Ø§Ù„Ø© Ø§Ù„Ø§Ø³ØªØ®Ù„Ø§Øµ Ø§Ù„Ù…Ø·Ù„Ù‚Ø© Ø§Ù„Ù…ÙØ·ÙˆÙ‘Ø±Ø© (V10.3 - Ù‚Ø§Ù‡Ø± Ø§Ù„Ù†ÙˆØ§ÙØ° Ø§Ù„Ù…Ù†Ø¨Ø«Ù‚Ø©) ---
async def get_pdf_link_from_page(link: str):
    """
    ØªØ³ØªØ®Ø¯Ù… Playwright Ø¨Ø®ÙŠØ§Ø±Ø§Øª ØªØ­ØµÙŠÙ† Ù…ØªÙ‚Ø¯Ù…Ø© ÙˆÙ…Ø³Ø§Ø± Ø§Ø³ØªÙ…Ø§Ø¹ Ù„Ù„Ù†Ø§ÙØ°Ø© Ø§Ù„Ù…Ù†Ø¨Ø«Ù‚Ø© (popup) 
    Ù„ØªØ¬Ø§ÙˆØ² Ø§Ù„ØªÙØ§Ø¹Ù„Ø§Øª Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø© ÙÙŠ Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹.
    """
    pdf_link = None
    page_title = "book" 
    browser = None 
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ø£ÙˆÙ„
    if link.lower().endswith('.pdf') or 'archive.org/download' in link.lower() or 'drive.google.com' in link.lower():
        return link, "Direct PDF"
        
    try:
        async with async_playwright() as p:
            # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ­ØµÙŠÙ† ÙˆÙ…Ø­Ø§ÙƒØ§Ø© Ø§Ù„Ø¬Ù‡Ø§Ø² Ø§Ù„Ù…Ø­Ù…ÙˆÙ„
            iphone_13 = p.devices['iPhone 13']
            
            browser = await p.chromium.launch(
                headless=True,
                args=[
                    '--no-sandbox', 
                    '--disable-setuid-sandbox',
                    '--disable-blink-features=AutomationControlled', 
                    f'--user-agent={iphone_13["user_agent"]}' 
                ]
            )
            context = await browser.new_context(**iphone_13) 
            page = await context.new_page()

            await page.goto(link, wait_until="domcontentloaded", timeout=40000) 
            
            html_content = await page.content()
            soup = BeautifulSoup(html_content, "html.parser")
            page_title = soup.title.string if soup.title else "book"
            download_selector_css = 'a[href*="pdf"], a.book-dl-btn, a.btn-download, button:has-text("ØªØ­Ù…ÙŠÙ„"), a:has-text("Download"), a:has-text("Ø§Ø¨Ø¯Ø£ Ø§Ù„ØªØ­Ù…ÙŠÙ„"), a:has-text("Ø§Ø¶ØºØ· Ù‡Ù†Ø§ Ù„Ù„ØªØ­Ù…ÙŠÙ„")'
            
            # --- Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª 2 Ùˆ 1 Ùˆ 4 Ùˆ 3 (Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©) ÙŠØªÙ… ØªÙ†ÙÙŠØ°Ù‡Ø§ Ù‡Ù†Ø§ ... ---
            
            # 1. Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø± Ø§Ù„Ø°ÙƒÙŠ (Strategy 2)
            try:
                await page.wait_for_selector('a[href$=".pdf"], a[href*="download"], a[href*="drive.google.com"]', timeout=10000)
                html_content = await page.content()
                soup = BeautifulSoup(html_content, "html.parser")
                
                for a_tag in soup.find_all('a', href=True):
                    href = urljoin(link, a_tag['href'])
                    if href.lower().endswith('.pdf') or 'download' in href.lower() or 'drive.google.com' in href.lower():
                        pdf_link = href
                        print(f"PDF link found via Smart Wait: {pdf_link}")
                        break
            except Exception:
                pass 
                
            if not pdf_link:
                
                # 2. Ø§Ù„ØªØ²Ø§Ù…Ù† (gather) (Strategy 1)
                try:
                    pdf_response, _ = await asyncio.gather(
                        page.wait_for_response(
                            lambda response: response.status in [200, 206, 301, 302] and (
                                'application/pdf' in response.headers.get('content-type', '') or 
                                response.url.lower().endswith('.pdf')
                            ),
                            timeout=30000
                        ),
                        page.click(download_selector_css, timeout=25000) 
                    )
                    pdf_link = pdf_response.url
                    
                except Exception:
                    
                    # 3. Ø§Ù„ØªÙ†Ù‚ÙŠØ¨ Ø§Ù„Ø´Ø¨ÙƒÙŠ Ø§Ù„Ø¹Ù…ÙŠÙ‚ (Strategy 4)
                    print("Executing Deep Network Mining.")
                    pdf_link = await fallback_strategy_4_network_mine(page, download_selector_css, link)
            
            # --- Ø§Ù„Ø§Ø¨ØªÙƒØ§Ø± 4: Ø§Ù„Ø§Ø³ØªÙ…Ø§Ø¹ Ù„Ù„Ù†Ø§ÙØ°Ø© Ø§Ù„Ù…Ù†Ø¨Ø«Ù‚Ø© (Strategy 5) ---
            if not pdf_link:
                print("All network/wait strategies failed. Attempting Popup Listener (Strategy 5).")
                
                try:
                    # Ù†Ù‚ÙˆÙ… Ø¨Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ù†Ù‚Ø± Ø¹Ù„Ù‰ Ø²Ø± Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ØŒ Ø¨Ø§Ù„ØªØ²Ø§Ù…Ù† Ù…Ø¹ Ø§Ù†ØªØ¸Ø§Ø± Ø¸Ù‡ÙˆØ± Ø§Ù„Ù†Ø§ÙØ°Ø© Ø§Ù„Ù…Ù†Ø¨Ø«Ù‚Ø©
                    popup_event = await asyncio.gather(
                        page.wait_for_event('popup', timeout=15000), 
                        page.click(download_selector_css, timeout=10000) 
                    )
                    
                    popup_page = popup_event[0]
                    await popup_page.wait_for_load_state("domcontentloaded")
                    
                    # ÙØ­Øµ Ø±Ø§Ø¨Ø· Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ù…Ù†Ø¨Ø«Ù‚Ø©
                    popup_url = popup_page.url.lower()
                    if popup_url.endswith('.pdf') or 'drive.google.com' in popup_url or 'dropbox.com' in popup_url:
                        pdf_link = popup_page.url
                        print(f"PDF link found via Popup Listener (Strategy 5): {pdf_link}")
                    
                    await popup_page.close()
                    
                except Exception as e:
                    print(f"Popup Listener failed: {e}")
                    
            
            # 4. ÙØ­Øµ HTML Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ (Strategy 3 - ÙƒÙ…Ø³Ø§Ø± Ø£Ø®ÙŠØ±)
            if not pdf_link:
                await asyncio.sleep(5) 
                final_html_content = await page.content()
                final_soup = BeautifulSoup(final_html_content, "html.parser")
                
                for a_tag in final_soup.find_all('a', href=True):
                    href = urljoin(link, a_tag['href'])
                    href_lower = href.lower()
                    
                    if href_lower.endswith('.pdf') or 'download' in href_lower:
                        pdf_link = href
                        print(f"General link found in HTML (Strategy 3 - Final): {pdf_link}")
                        break

            # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
            if not page_title:
                 html_content = await page.content()
                 soup = BeautifulSoup(html_content, "html.parser")
                 page_title = soup.title.string if soup.title else "book"

            return pdf_link, page_title
    
    except Exception as e:
        print(f"Critical error in get_pdf_link_from_page: {e}")
        raise e
    
    finally:
        if browser:
            await browser.close()


# --- Ø¯ÙˆØ§Ù„ ØªÙŠÙ„ÙŠØ¬Ø±Ø§Ù… (download_and_send_pdfØŒ startØŒ search_cmdØŒ callback_handlerØŒ main) ---
async def download_and_send_pdf(context, chat_id, pdf_url, title="book.pdf"):
    """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„ÙØŒ Ø¥Ø±Ø³Ø§Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ØŒ Ø«Ù… Ø­Ø°ÙÙ‡ Ù…Ù† Ø§Ù„Ù‚Ø±Øµ Ø§Ù„ØµÙ„Ø¨."""
    tmp_dir = tempfile.gettempdir()
    file_path = os.path.join(tmp_dir, title.replace("/", "_")[:40] + ".pdf")
    
    async with ClientSession() as session:
        async with session.get(pdf_url, headers=USER_AGENT_HEADER) as resp:
            if resp.status != 200:
                await context.bot.send_message(
                    chat_id=chat_id, 
                    text=f"âš ï¸ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù Ù…Ù† Ø§Ù„Ù…ØµØ¯Ø±. Ø±Ù…Ø² Ø§Ù„Ø®Ø·Ø£: {resp.status}"
                )
                return
            
            content = await resp.read()

            if len(content) < MIN_PDF_SIZE_BYTES:
                await context.bot.send_message(
                    chat_id=chat_id, 
                    text="âš ï¸ ÙØ´Ù„ Ø§Ù„ØªØ­Ù…ÙŠÙ„: Ø­Ø¬Ù… Ø§Ù„Ù…Ù„Ù ØµØºÙŠØ± Ø¬Ø¯Ø§Ù‹ (ØºÙŠØ± ØµØ§Ù„Ø­). Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ø§Ù„Ø±Ø§Ø¨Ø· Ø®Ø§Ø·Ø¦Ø§Ù‹."
                )
                return
            
            async with aiofiles.open(file_path, "wb") as f:
                await f.write(content)
            
            try:
                with open(file_path, "rb") as f:
                    await context.bot.send_document(
                        chat_id=chat_id, 
                        document=f
                    )
                await context.bot.send_message(chat_id=chat_id, text="âœ… ØªÙ… Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„ÙƒØªØ§Ø¨ Ø¨Ù†Ø¬Ø§Ø­.")
            except Exception as e:
                 await context.bot.send_message(chat_id=chat_id, text=f"âš ï¸ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù…Ù„Ù Ø¥Ù„Ù‰ ØªÙŠÙ„ÙŠØ¬Ø±Ø§Ù…: {e}")
            finally:
                if os.path.exists(file_path):
                    os.remove(file_path)
                
# --- Ø¯ÙˆØ§Ù„ Ø£ÙˆØ§Ù…Ø± ØªÙŠÙ„ÙŠØ¬Ø±Ø§Ù… (Telegram Commands) ---

async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text(
        "ğŸ“š Ø¨ÙˆØª Ø§Ù„Ù‚ÙŠØ§Ù…Ø© Ø¬Ø§Ù‡Ø²!\n"
        "Ø£Ø±Ø³Ù„ /search Ù…ØªØ¨ÙˆØ¹Ù‹Ø§ Ø¨Ø§Ø³Ù… Ø§Ù„ÙƒØªØ§Ø¨ Ø£Ùˆ Ø§Ù„Ù…Ø¤Ù„Ù."
    )

async def search_cmd(update: Update, context: ContextTypes.DEFAULT_TYPE):
    query = " ".join(context.args).strip()
    if not query:
        await update.message.reply_text("Ø§Ø³ØªØ®Ø¯Ù…: /search Ø§Ø³Ù… Ø§Ù„ÙƒØªØ§Ø¨ Ø£Ùˆ Ø§Ù„Ù…Ø¤Ù„Ù")
        return

    msg = await update.message.reply_text(f"ğŸ” Ø£Ø¨Ø­Ø« Ø¹Ù† **{query}** Ø¹Ø¨Ø± **DuckDuckGo** (ÙÙ„ØªØ±Ø© ØµØ§Ø±Ù…Ø© Ù„Ù„Ù†ØªØ§Ø¦Ø¬)...")
    
    try:
        results = await search_duckduckgo(query)

        if not results:
            await msg.edit_text("âŒ Ù„Ù… Ø£Ø¬Ø¯ Ù†ØªØ§Ø¦Ø¬ Ù…ÙˆØ«ÙˆÙ‚Ø© ÙÙŠ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù…Ø®ØªØ§Ø±Ø©. Ø­Ø§ÙˆÙ„ Ø¨ÙƒÙ„Ù…Ø§Øª Ù…Ø®ØªÙ„ÙØ© Ø£Ùˆ Ø¬Ø±Ø¨ Ø§Ù„Ø¨Ø­Ø« Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.")
            return

        buttons = []
        text_lines = []
        
        context.user_data[TEMP_LINKS_KEY] = [item.get("link") for item in results]
        
        for i, item in enumerate(results, start=0):
            title = item.get("title")[:120]
            source = next((d.replace('.com', '').replace('.net', '') for d in TRUSTED_DOMAINS if d in item.get('link')), "Ù…Ø¨Ø§Ø´Ø±/Ø¹Ø§Ù…")
            text_lines.append(f"{i+1}. {title} (Ø§Ù„Ù…ØµØ¯Ø±: {source})")
            buttons.append([InlineKeyboardButton(f"ğŸ“¥ ØªØ­Ù…ÙŠÙ„ {i+1}", callback_data=f"dl|{i}")])
            
        reply = "\n".join(text_lines)
        await msg.edit_text(reply, reply_markup=InlineKeyboardMarkup(buttons))
        
    except Exception as e:
         await msg.edit_text(f"âš ï¸ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø¨Ø­Ø«: {e}")


async def callback_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    query = update.callback_query
    await query.answer()
    data = query.data
    
    if data.startswith("dl|"):
        try:
            index_str = data.split("|", 1)[1]
            index = int(index_str)
            link = context.user_data[TEMP_LINKS_KEY][index]

        except Exception:
            await context.bot.send_message(
                chat_id=query.message.chat_id,
                text="âš ï¸ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø²Ø± Ø§Ù„ØªØ­Ù…ÙŠÙ„ (Ø±Ø§Ø¨Ø· ØºÙŠØ± ØµØ§Ù„Ø­). ÙŠØ±Ø¬Ù‰ Ø§Ù„Ø¨Ø­Ø« Ù…Ø¬Ø¯Ø¯Ø§Ù‹.",
            )
            return
            
        await query.edit_message_text("â³ ØªÙØ¹ÙŠÙ„ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ø§Ø³ØªØ®Ù„Ø§Øµ Ø§Ù„Ù†Ø§Ø±ÙŠ (V10.3 - Ù‚Ø§Ù‡Ø± Ø§Ù„Ù†ÙˆØ§ÙØ° Ø§Ù„Ù…Ù†Ø¨Ø«Ù‚Ø©)...")
        
        try:
            pdf_link, title = await get_pdf_link_from_page(link)
            
            if pdf_link:
                await download_and_send_pdf(context, query.message.chat_id, pdf_link, title=title if title else "book")
            else:
                await context.bot.send_message(
                    chat_id=query.message.chat_id,
                    text=f"ğŸ“„ ÙØ´Ù„ Ø§Ù„Ø§Ø³ØªØ®Ù„Ø§Øµ. Ù‚Ø¯ ØªÙƒÙˆÙ† Ø§Ù„Ø­Ù…Ø§ÙŠØ© Ù‚ÙˆÙŠØ© Ø¬Ø¯Ø§Ù‹. Ø±Ø§Ø¨Ø· Ø§Ù„Ù…ØµØ¯Ø±: {link}",
                )
        
        except Exception as e:
            await context.bot.send_message(
                chat_id=query.message.chat_id,
                text=f"âš ï¸ Ø®Ø·Ø£ Playwright Ø£Ø«Ù†Ø§Ø¡ Ø¬Ù„Ø¨ Ø§Ù„Ù…Ù„Ù: {e}",
            )


def main():
    if not BOT_TOKEN:
        raise ValueError("BOT_TOKEN is missing in environment variables.")

    app = ApplicationBuilder().token(BOT_TOKEN).build()

    app.add_handler(CommandHandler("start", start))
    app.add_handler(CommandHandler("search", search_cmd))
    app.add_handler(CallbackQueryHandler(callback_handler))

    print("Ø§Ù„Ø¨ÙˆØª Ø¨Ø¯Ø£ Ø§Ù„Ø¹Ù…Ù„.")
    app.run_polling()

if __name__ == "__main__":
    main()
