import os
import asyncio
import tempfile
import aiofiles
from aiohttp import ClientSession
from bs4 import BeautifulSoup
from telegram import InlineKeyboardButton, InlineKeyboardMarkup
from telegram.ext import ApplicationBuilder, CommandHandler, CallbackQueryHandler, ContextTypes 
from playwright.async_api import async_playwright, Page 
from urllib.parse import urljoin 
from ddgs import DDGS # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… ddgs ÙÙ‚Ø·

# --- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø¨ÙˆØª ÙˆØ§Ù„Ø«ÙˆØ§Ø¨Øª ---
BOT_TOKEN = os.getenv("BOT_TOKEN")

USER_AGENT_HEADER = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
MIN_PDF_SIZE_BYTES = 50 * 1024 
TEMP_LINKS_KEY = "current_search_links" 
# Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø£Ù‚Ù„ Ø­Ù…Ø§ÙŠØ© ÙÙ‚Ø·
TRUSTED_DOMAINS = [
    "kotobati.com", 
    "masaha.org", 
    "books-library.net"
]

# --- Ø¯Ø§Ù„Ø© Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø«ÙˆØ±ÙŠØ© (DuckDuckGo) ---
async def search_duckduckgo(query: str):
    """ÙŠØ³ØªØ®Ø¯Ù… DuckDuckGo API Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø±ÙˆØ§Ø¨Ø· PDF Ù…Ø¨Ø§Ø´Ø±Ø© ÙÙŠ Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„Ù…ÙˆØ«ÙˆÙ‚Ø©."""
    
    sites_query = " OR ".join([f"site:{d}" for d in TRUSTED_DOMAINS])
    full_query = f"{query} filetype:pdf OR {sites_query}"
    
    print(f"Executing search query: {full_query}")
    
    results = []
    
    # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… DDGS Ø¨Ø§Ù„Ø´ÙƒÙ„ Ø§Ù„ØµØ­ÙŠØ­
    with DDGS(timeout=5) as ddgs:
        search_results = ddgs.text(full_query, max_results=10)
        
        for r in search_results:
            link = r.get("href")
            title = r.get("title")
            
            if any(d in link for d in TRUSTED_DOMAINS) or link.lower().endswith(".pdf"):
                results.append({"title": title, "link": link})

    unique_links = {}
    for item in results:
        unique_links[item['link']] = item
    
    return list(unique_links.values())[:5]

# --- Ø§Ù„Ø¥Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ø±Ø§Ø¨Ø¹Ø© Ø§Ù„Ù…Ø¨ØªÙƒØ±Ø©: Ø§Ù„ØªÙ†Ù‚ÙŠØ¨ ÙÙŠ Ø¬Ù…ÙŠØ¹ Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø´Ø¨ÙƒØ© ---
async def fallback_strategy_4_network_mine(page: Page, download_selector_css: str, link: str):
    
    network_urls = set()

    def capture_url(response):
        if response.status in [200, 206, 301, 302]:
            network_urls.add(response.url)
            
    page.on("response", capture_url)
    
    try:
        await page.locator(download_selector_css).click(timeout=15000) 
        await asyncio.sleep(7) 
        
        for url in network_urls:
            url_lower = url.lower()
            if url_lower.endswith('.pdf') or 'drive.google.com' in url_lower or 'dropbox.com' in url_lower or 'archive.org/download' in url_lower:
                print(f"PDF link found via Network Mining: {url}")
                return url
        
        return None 
        
    except Exception as e:
        print(f"Network mining failed: {e}")
        return None
        
    finally:
        try:
            page.remove_listener("response", capture_url)
        except:
            pass 

# --- Ø¯Ø§Ù„Ø© Ø§Ù„Ø§Ø³ØªØ®Ù„Ø§Øµ Ø§Ù„Ù…Ø·Ù„Ù‚Ø© Ø§Ù„Ù…ÙØ­Ø³ÙÙ‘Ù†Ø© (V7.0) ---
async def get_pdf_link_from_page(link: str):
    """
    ÙŠØ³ØªØ®Ø¯Ù… Playwright Ù„Ù…Ø­Ø§ÙƒØ§Ø© Ø§Ù„Ø¶ØºØ· ÙˆÙŠÙ†ØªØ¸Ø± Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø´Ø¨ÙƒØ© ØªØ­Ù…Ù„ Ù…Ù„Ù PDF.
    """
    pdf_link = None
    page_title = "book" 
    browser = None 
    
    # ğŸ’¥ Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ø£ÙˆÙ„: Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø±Ø§Ø¨Ø· Ù…Ø¨Ø§Ø´Ø±Ø§Ù‹ØŒ Ù„Ø§ Ø¯Ø§Ø¹ÙŠ Ù„Ù€ Playwright
    if link.lower().endswith('.pdf') or 'archive.org/download' in link.lower() or 'drive.google.com' in link.lower():
        print(f"Direct PDF link detected. Bypassing Playwright: {link}")
        return link, "Direct PDF"
        
    try:
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page()
            
            await page.goto(link, wait_until="domcontentloaded", timeout=40000) 
            
            html_content = await page.content()
            soup = BeautifulSoup(html_content, "html.parser")
            page_title = soup.title.string if soup.title else "book"
            
            download_selector_css = 'a[href*="pdf"], a.book-dl-btn, a.btn-download, button:has-text("ØªØ­Ù…ÙŠÙ„"), a:has-text("Download"), a:has-text("Ø§Ø¨Ø¯Ø£ Ø§Ù„ØªØ­Ù…ÙŠÙ„"), a:has-text("Ø§Ø¶ØºØ· Ù‡Ù†Ø§ Ù„Ù„ØªØ­Ù…ÙŠÙ„")'
            
            # --- Ù…Ø­Ø§ÙˆÙ„Ø© 1: Ø§Ù„ØªØ²Ø§Ù…Ù† (gather) ---
            try:
                pdf_response, _ = await asyncio.gather(
                    page.wait_for_response(
                        lambda response: response.status in [200, 206, 301, 302] and (
                            'application/pdf' in response.headers.get('content-type', '') or 
                            response.url.lower().endswith('.pdf')
                        ),
                        timeout=30000
                    ),
                    page.click(download_selector_css, timeout=25000) 
                )
                
                pdf_link = pdf_response.url
                
            except Exception as e:
                print(f"Initial gather failed, attempting fallback strategies: {e}")
                
                # --- Ù…Ø­Ø§ÙˆÙ„Ø© 2: Ø§Ù„Ù†Ù‚Ø± Ø«Ù… Ø§Ù„ØªØ£Ø®ÙŠØ± Ø«Ù… Ø§Ù„ØªÙ†ØµØª ---
                try:
                    await page.click(download_selector_css, timeout=25000) 
                    await asyncio.sleep(4)
                    
                    pdf_response = await page.wait_for_response(
                         lambda response: response.status in [200, 206, 301, 302] and (
                            'application/pdf' in response.headers.get('content-type', '') or 
                            response.url.lower().endswith('.pdf')
                        ),
                        timeout=10000 
                    )
                    pdf_link = pdf_response.url
                    
                except Exception as fallback_error:
                    print(f"Second fallback failed, checking HTML (Strategy 3): {fallback_error}")
                    
                    # --- Ù…Ø­Ø§ÙˆÙ„Ø© 3: ÙØ­Øµ HTML Ø¨Ø¹Ø¯ Ø§Ù„Ù†Ù‚Ø± ÙˆØ§Ù„ØªØ£Ø®ÙŠØ± ---
                    await asyncio.sleep(5) 
                    final_html_content = await page.content()
                    final_soup = BeautifulSoup(final_html_content, "html.parser")
                    
                    for a_tag in final_soup.find_all('a', href=True):
                        href = urljoin(link, a_tag['href'])
                        href_lower = href.lower()
                        
                        if href_lower.endswith('.pdf'):
                            pdf_link = href
                            print(f"PDF link found in HTML (Strategy 3): {pdf_link}")
                            break
                        
                    if not pdf_link:
                         for a_tag in final_soup.find_all('a', href=True):
                            href = urljoin(link, a_tag['href'])
                            href_lower = href.lower()

                            if 'download' in href_lower or 'drive.google.com' in href_lower or 'dropbox.com' in href_lower or 'archive.org/download' in href_lower:
                                pdf_link = href
                                print(f"General download link found in HTML (Strategy 3): {pdf_link}")
                                break
                    
                    # --- Ù…Ø­Ø§ÙˆÙ„Ø© 4 (Ø§Ù„Ø£Ø®ÙŠØ±Ø©): Ø§Ù„ØªÙ†Ù‚ÙŠØ¨ ÙÙŠ Ø§Ù„Ø´Ø¨ÙƒØ© ---
                    if not pdf_link:
                         print("HTML check failed. Executing Network Mining (Strategy 4).")
                         pdf_link = await fallback_strategy_4_network_mine(page, download_selector_css, link)


            return pdf_link, page_title
    
    except Exception as e:
        print(f"Critical error in get_pdf_link_from_page: {e}")
        raise e
    
    finally:
        if 'page' in locals():
            try:
                await page.close()
            except:
                pass
        if browser:
            await browser.close()
            print("ØªÙ… Ø¶Ù…Ø§Ù† Ø¥ØºÙ„Ø§Ù‚ Ù…ØªØµÙØ­ Playwright.")


# --- Ø¨Ø§Ù‚ÙŠ Ø¯ÙˆØ§Ù„ ØªÙŠÙ„ÙŠØ¬Ø±Ø§Ù… (download_and_send_pdfØŒ startØŒ search_cmdØŒ callback_handlerØŒ main) ---
async def download_and_send_pdf(context, chat_id, pdf_url, title="book.pdf"):
    """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„ÙØŒ Ø¥Ø±Ø³Ø§Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ØŒ Ø«Ù… Ø­Ø°ÙÙ‡ Ù…Ù† Ø§Ù„Ù‚Ø±Øµ Ø§Ù„ØµÙ„Ø¨."""
    tmp_dir = tempfile.gettempdir()
    file_path = os.path.join(tmp_dir, title.replace("/", "_")[:40] + ".pdf")
    
    async with ClientSession() as session:
        async with session.get(pdf_url, headers=USER_AGENT_HEADER) as resp:
            if resp.status != 200:
                await context.bot.send_message(
                    chat_id=chat_id, 
                    text=f"âš ï¸ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù Ù…Ù† Ø§Ù„Ù…ØµØ¯Ø±. Ø±Ù…Ø² Ø§Ù„Ø®Ø·Ø£: {resp.status}"
                )
                return
            
            content = await resp.read()

            if len(content) < MIN_PDF_SIZE_BYTES:
                await context.bot.send_message(
                    chat_id=chat_id, 
                    text="âš ï¸ ÙØ´Ù„ Ø§Ù„ØªØ­Ù…ÙŠÙ„: Ø­Ø¬Ù… Ø§Ù„Ù…Ù„Ù ØµØºÙŠØ± Ø¬Ø¯Ø§Ù‹ (ØºÙŠØ± ØµØ§Ù„Ø­). Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ø§Ù„Ø±Ø§Ø¨Ø· Ø®Ø§Ø·Ø¦Ø§Ù‹."
                )
                return
            
            async with aiofiles.open(file_path, "wb") as f:
                await f.write(content)
            
            try:
                with open(file_path, "rb") as f:
                    await context.bot.send_document(
                        chat_id=chat_id, 
                        document=f
                    )
                await context.bot.send_message(chat_id=chat_id, text="âœ… ØªÙ… Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„ÙƒØªØ§Ø¨ Ø¨Ù†Ø¬Ø§Ø­.")
            except Exception as e:
                 await context.bot.send_message(chat_id=chat_id, text=f"âš ï¸ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù…Ù„Ù Ø¥Ù„Ù‰ ØªÙŠÙ„ÙŠØ¬Ø±Ø§Ù…: {e}")
            finally:
                if os.path.exists(file_path):
                    os.remove(file_path)
                
# --- Ø¯ÙˆØ§Ù„ Ø£ÙˆØ§Ù…Ø± ØªÙŠÙ„ÙŠØ¬Ø±Ø§Ù… (Telegram Commands) ---

async def start(update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text(
        "ğŸ“š Ø¨ÙˆØª Ø§Ù„Ù‚ÙŠØ§Ù…Ø© Ø¬Ø§Ù‡Ø²!\n"
        "Ø£Ø±Ø³Ù„ /search Ù…ØªØ¨ÙˆØ¹Ù‹Ø§ Ø¨Ø§Ø³Ù… Ø§Ù„ÙƒØªØ§Ø¨ Ø£Ùˆ Ø§Ù„Ù…Ø¤Ù„Ù."
    )

async def search_cmd(update, context: ContextTypes.DEFAULT_TYPE):
    query = " ".join(context.args).strip()
    if not query:
        await update.message.reply_text("Ø§Ø³ØªØ®Ø¯Ù…: /search Ø§Ø³Ù… Ø§Ù„ÙƒØªØ§Ø¨ Ø£Ùˆ Ø§Ù„Ù…Ø¤Ù„Ù")
        return

    msg = await update.message.reply_text("ğŸ” Ø£Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙƒØªØ§Ø¨ Ø¹Ø¨Ø± DuckDuckGo (ØºÙŠØ± Ù…Ù‚ÙŠØ¯)...")
    
    try:
        results = await search_duckduckgo(query)

        if not results:
            await msg.edit_text("âŒ Ù„Ù… Ø£Ø¬Ø¯ Ù†ØªØ§Ø¦Ø¬ Ù…ÙˆØ«ÙˆÙ‚Ø© ÙÙŠ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù…Ø®ØªØ§Ø±Ø©. Ø­Ø§ÙˆÙ„ Ø¨ÙƒÙ„Ù…Ø§Øª Ù…Ø®ØªÙ„ÙØ©.")
            return

        buttons = []
        text_lines = []
        
        context.user_data[TEMP_LINKS_KEY] = [item.get("link") for item in results]
        
        for i, item in enumerate(results, start=0):
            title = item.get("title")[:120]
            source = next((d.replace('.com', '').replace('.net', '') for d in TRUSTED_DOMAINS if d in item.get('link')), "Ø±Ø§Ø¨Ø· Ù…Ø¨Ø§Ø´Ø±")
            text_lines.append(f"{i+1}. {title} (Ø§Ù„Ù…ØµØ¯Ø±: {source})")
            buttons.append([InlineKeyboardButton(f"ğŸ“¥ ØªØ­Ù…ÙŠÙ„ {i+1}", callback_data=f"dl|{i}")])
            
        reply = "\n".join(text_lines)
        await msg.edit_text(reply, reply_markup=InlineKeyboardMarkup(buttons))
        
    except Exception as e:
         await msg.edit_text(f"âš ï¸ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø¨Ø­Ø«: {e}")


async def callback_handler(update, context: ContextTypes.DEFAULT_TYPE):
    query = update.callback_query
    await query.answer()
    data = query.data
    
    if data.startswith("dl|"):
        try:
            index_str = data.split("|", 1)[1]
            index = int(index_str)
            link = context.user_data[TEMP_LINKS_KEY][index]

        except Exception:
            await context.bot.send_message(
                chat_id=query.message.chat_id,
                text="âš ï¸ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø²Ø± Ø§Ù„ØªØ­Ù…ÙŠÙ„ (Ø±Ø§Ø¨Ø· ØºÙŠØ± ØµØ§Ù„Ø­). ÙŠØ±Ø¬Ù‰ Ø§Ù„Ø¨Ø­Ø« Ù…Ø¬Ø¯Ø¯Ø§Ù‹.",
            )
            return
            
        await query.edit_message_text("â³ ØªÙØ¹ÙŠÙ„ Ø§Ù„ØªÙ†ØµØª Ø¹Ù„Ù‰ Ù†ÙˆØ¹ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ (MIME Type) Ù„Ø¹Ø¨ÙˆØ± Ø§Ù„Ø­Ù…Ø§ÙŠØ©...")
        
        try:
            pdf_link, title = await get_pdf_link_from_page(link)
            
            if pdf_link:
                await download_and_send_pdf(context, query.message.chat_id, pdf_link, title=title if title else "book")
            else:
                await context.bot.send_message(
                    chat_id=query.message.chat_id,
                    text=f"ğŸ“„ ÙØ´Ù„ Ø§Ù„Ø§Ø³ØªØ®Ù„Ø§Øµ. Ù‚Ø¯ ØªÙƒÙˆÙ† Ø§Ù„Ø­Ù…Ø§ÙŠØ© Ù‚ÙˆÙŠØ© Ø¬Ø¯Ø§Ù‹. Ø±Ø§Ø¨Ø· Ø§Ù„Ù…ØµØ¯Ø±: {link}",
                )
        
        except Exception as e:
            await context.bot.send_message(
                chat_id=query.message.chat_id,
                text=f"âš ï¸ Ø®Ø·Ø£ Playwright Ø£Ø«Ù†Ø§Ø¡ Ø¬Ù„Ø¨ Ø§Ù„Ù…Ù„Ù: {e}",
            )


def main():
    if not BOT_TOKEN:
        raise ValueError("BOT_TOKEN is missing in environment variables.")

    app = ApplicationBuilder().token(BOT_TOKEN).build()

    app.add_handler(CommandHandler("start", start))
    app.add_handler(CommandHandler("search", search_cmd))
    app.add_handler(CallbackQueryHandler(callback_handler))

    print("Ø§Ù„Ø¨ÙˆØª Ø¨Ø¯Ø£ Ø§Ù„Ø¹Ù…Ù„.")
    app.run_polling()

if __name__ == "__main__":
    main()
