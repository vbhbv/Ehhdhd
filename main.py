import os
import asyncio
import tempfile
import aiofiles
from aiohttp import ClientSession
from bs4 import BeautifulSoup
from telegram import InlineKeyboardButton, InlineKeyboardMarkup, Update
from telegram.ext import ApplicationBuilder, CommandHandler, CallbackQueryHandler, ContextTypes 
from playwright.async_api import async_playwright, Page 
from urllib.parse import urljoin 

# --- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø¨ÙˆØª ÙˆØ§Ù„Ø«ÙˆØ§Ø¨Øª ---
BOT_TOKEN = os.getenv("BOT_TOKEN")

USER_AGENT_HEADER = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
MIN_PDF_SIZE_BYTES = 50 * 1024 
TEMP_LINKS_KEY = "current_search_links" 
TRUSTED_DOMAINS = [
    "kotobati.com", 
    "masaha.org", # Ù„Ù… ØªÙØ¶Ø§Ù ÙƒØ¯Ø§Ù„Ø© Ø¨Ø­Ø« Ù…ØªØ®ØµØµØ© Ù„Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ Ù†Ù…Ø· ÙˆØ§Ø¶Ø­
    "archive.org"
]

# ğŸ’¥ Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…Ø®ØµØµ (V9.0) - ØªÙ… Ø¥Ù„ØºØ§Ø¡ DDGS
SITE_SEARCH_PATTERNS = {
    "kotobati.com": "https://kotobati.com/search?q={query}",
    "archive.org": "https://archive.org/details/texts?query={query}",
    # ÙŠÙ…ÙƒÙ† Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø²ÙŠØ¯ Ù‡Ù†Ø§ (Ù…Ø«Ù„ Masaha)
}

# --- Ø¯Ø§Ù„Ø© Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…Ø®ØµØµ Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© (V9.0) ---
async def search_site_and_extract_links(query: str):
    """
    ÙŠÙ‚ÙˆÙ… Ø¨Ø§Ù„Ø¨Ø­Ø« Ù…Ø¨Ø§Ø´Ø±Ø© Ø¯Ø§Ø®Ù„ Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„Ù…ÙˆØ«ÙˆÙ‚Ø© ÙˆÙŠØ³ØªØ®Ù„Øµ Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ÙƒØªØ¨ Ø§Ù„ÙØ±Ø¯ÙŠØ©.
    """
    results = []
    
    try:
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page()

            for domain, url_pattern in SITE_SEARCH_PATTERNS.items():
                search_url = url_pattern.format(query=query)
                
                try:
                    print(f"Searching {domain} at: {search_url}")
                    await page.goto(search_url, wait_until="domcontentloaded", timeout=30000)
                    html_content = await page.content()
                    soup = BeautifulSoup(html_content, "html.parser")

                    if "kotobati.com" in domain:
                        # Ù…Ø­Ø¯Ø¯Ø§Øª Kotobati (Ù‚Ø¯ ØªØ­ØªØ§Ø¬ Ù„ØªØ¹Ø¯ÙŠÙ„ Ø¨Ø³ÙŠØ· Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù‡ÙŠÙƒÙ„ÙŠØ© Ø§Ù„Ø­Ø§Ù„ÙŠØ©)
                        book_cards = soup.select('.book-item a') 
                        for card in book_cards[:3]: 
                            link = urljoin(url_pattern, card.get('href'))
                            title_tag = card.select_one('.book-title')
                            if title_tag and link:
                                 results.append({"title": title_tag.text.strip(), "link": link})

                    elif "archive.org" in domain:
                        # Ù…Ø­Ø¯Ø¯Ø§Øª Archive.org
                        item_links = soup.select('.item-ttl a')
                        for link_tag in item_links[:3]:
                            link = urljoin(url_pattern, link_tag.get('href'))
                            title = link_tag.text.strip()
                            results.append({"title": title, "link": link})
                            
                    if len(results) >= 6:
                        break

                except Exception as e:
                    print(f"Error searching {domain}: {e}")
                    continue

            await browser.close()
            
    except Exception as e:
        print(f"Playwright initiation failed during search: {e}")
    
    # Ø¶Ù…Ø§Ù† Ø¹Ø¯Ù… ØªÙƒØ±Ø§Ø± Ø§Ù„Ø±ÙˆØ§Ø¨Ø· ÙˆØ§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£ÙØ¶Ù„ 5 Ù†ØªØ§Ø¦Ø¬
    unique_links = {}
    for item in results:
        unique_links[item['link']] = item
        
    return list(unique_links.values())[:5]


# --- Ø§Ù„Ø¥Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ø±Ø§Ø¨Ø¹Ø© Ø§Ù„Ù…Ø¨ØªÙƒØ±Ø©: Ø§Ù„ØªÙ†Ù‚ÙŠØ¨ ÙÙŠ Ø¬Ù…ÙŠØ¹ Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø´Ø¨ÙƒØ© ---
async def fallback_strategy_4_network_mine(page: Page, download_selector_css: str, link: str):
    
    network_urls = set()

    def capture_url(response):
        if response.status in [200, 206, 301, 302]:
            network_urls.add(response.url)
            
    page.on("response", capture_url)
    
    try:
        await page.locator(download_selector_css).click(timeout=15000) 
        await asyncio.sleep(7) 
        
        for url in network_urls:
            url_lower = url.lower()
            if url_lower.endswith('.pdf') or 'drive.google.com' in url_lower or 'dropbox.com' in url_lower or 'archive.org/download' in url_lower:
                print(f"PDF link found via Network Mining: {url}")
                return url
        
        return None 
        
    except Exception as e:
        print(f"Network mining failed: {e}")
        return None
        
    finally:
        try:
            page.remove_listener("response", capture_url)
        except:
            pass 

# --- Ø¯Ø§Ù„Ø© Ø§Ù„Ø§Ø³ØªØ®Ù„Ø§Øµ Ø§Ù„Ù…Ø·Ù„Ù‚Ø© (V7.0) ---
async def get_pdf_link_from_page(link: str):
    """
    ÙŠØ³ØªØ®Ø¯Ù… Playwright Ù„Ù…Ø­Ø§ÙƒØ§Ø© Ø§Ù„Ø¶ØºØ· ÙˆÙŠÙ†ØªØ¸Ø± Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø´Ø¨ÙƒØ© ØªØ­Ù…Ù„ Ù…Ù„Ù PDF.
    """
    pdf_link = None
    page_title = "book" 
    browser = None 
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ø£ÙˆÙ„: Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø±Ø§Ø¨Ø· Ù…Ø¨Ø§Ø´Ø±Ø§Ù‹ØŒ Ù„Ø§ Ø¯Ø§Ø¹ÙŠ Ù„Ù€ Playwright
    if link.lower().endswith('.pdf') or any(d in link.lower() for d in ['archive.org/download', 'drive.google.com']):
        print(f"Direct PDF link detected. Bypassing Playwright: {link}")
        return link, "Direct PDF"
        
    try:
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page()
            
            await page.goto(link, wait_until="domcontentloaded", timeout=40000) 
            
            html_content = await page.content()
            soup = BeautifulSoup(html_content, "html.parser")
            page_title = soup.title.string if soup.title else "book"
            
            download_selector_css = 'a[href*="pdf"], a.book-dl-btn, a.btn-download, button:has-text("ØªØ­Ù…ÙŠÙ„"), a:has-text("Download"), a:has-text("Ø§Ø¨Ø¯Ø£ Ø§Ù„ØªØ­Ù…ÙŠÙ„"), a:has-text("Ø§Ø¶ØºØ· Ù‡Ù†Ø§ Ù„Ù„ØªØ­Ù…ÙŠÙ„")'
            
            # --- Ù…Ø­Ø§ÙˆÙ„Ø© 1: Ø§Ù„ØªØ²Ø§Ù…Ù† (gather) ---
            try:
                pdf_response, _ = await asyncio.gather(
                    page.wait_for_response(
                        lambda response: response.status in [200, 206, 301, 302] and (
                            'application/pdf' in response.headers.get('content-type', '') or 
                            response.url.lower().endswith('.pdf')
                        ),
                        timeout=30000
                    ),
                    page.click(download_selector_css, timeout=25000) 
                )
                
                pdf_link = pdf_response.url
                
            except Exception as e:
                print(f"Initial gather failed, attempting fallback strategies: {e}")
                
                # --- Ù…Ø­Ø§ÙˆÙ„Ø© 2 Ùˆ 3 Ùˆ 4 (Ø§Ù„Ø¨Ù‚ÙŠØ©) ---
                
                # ... (Ù‡Ù†Ø§ ÙŠØªÙ… Ø¥Ø¯Ø±Ø§Ø¬ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø§Øª 2 Ùˆ 3 Ùˆ 4 Ø§Ù„Ù…ØªØ¨Ù‚ÙŠØ©ØŒ ÙˆØ§Ù„ØªÙŠ Ù„Ù… ØªØªØºÙŠØ± Ø¹Ù† V7.0) ...
                
                # --- Ù…Ø­Ø§ÙˆÙ„Ø© 2: Ø§Ù„Ù†Ù‚Ø± Ø«Ù… Ø§Ù„ØªØ£Ø®ÙŠØ± Ø«Ù… Ø§Ù„ØªÙ†ØµØª ---
                try:
                    await page.click(download_selector_css, timeout=25000) 
                    await asyncio.sleep(4)
                    
                    pdf_response = await page.wait_for_response(
                         lambda response: response.status in [200, 206, 301, 302] and (
                            'application/pdf' in response.headers.get('content-type', '') or 
                            response.url.lower().endswith('.pdf')
                        ),
                        timeout=10000 
                    )
                    pdf_link = pdf_response.url
                    
                except Exception as fallback_error:
                    print(f"Second fallback failed, checking HTML (Strategy 3): {fallback_error}")
                    
                    # --- Ù…Ø­Ø§ÙˆÙ„Ø© 3: ÙØ­Øµ HTML Ø¨Ø¹Ø¯ Ø§Ù„Ù†Ù‚Ø± ÙˆØ§Ù„ØªØ£Ø®ÙŠØ± ---
                    await asyncio.sleep(5) 
                    final_html_content = await page.content()
                    final_soup = BeautifulSoup(final_html_content, "html.parser")
                    
                    for a_tag in final_soup.find_all('a', href=True):
                        href = urljoin(link, a_tag['href'])
                        href_lower = href.lower()
                        
                        if href_lower.endswith('.pdf'):
                            pdf_link = href
                            print(f"PDF link found in HTML (Strategy 3): {pdf_link}")
                            break
                        
                    if not pdf_link:
                         for a_tag in final_soup.find_all('a', href=True):
                            href = urljoin(link, a_tag['href'])
                            href_lower = href.lower()

                            if 'download' in href_lower or 'drive.google.com' in href_lower or 'dropbox.com' in href_lower or 'archive.org/download' in href_lower:
                                pdf_link = href
                                print(f"General download link found in HTML (Strategy 3): {pdf_link}")
                                break
                    
                    # --- Ù…Ø­Ø§ÙˆÙ„Ø© 4 (Ø§Ù„Ø£Ø®ÙŠØ±Ø©): Ø§Ù„ØªÙ†Ù‚ÙŠØ¨ ÙÙŠ Ø§Ù„Ø´Ø¨ÙƒØ© ---
                    if not pdf_link:
                         print("HTML check failed. Executing Network Mining (Strategy 4).")
                         pdf_link = await fallback_strategy_4_network_mine(page, download_selector_css, link)
                
            # ... (Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø§Øª 2 Ùˆ 3 Ùˆ 4) ...


            return pdf_link, page_title
    
    except Exception as e:
        print(f"Critical error in get_pdf_link_from_page: {e}")
        raise e
    
    finally:
        if 'page' in locals():
            try:
                await page.close()
            except:
                pass
        if browser:
            await browser.close()
            print("ØªÙ… Ø¶Ù…Ø§Ù† Ø¥ØºÙ„Ø§Ù‚ Ù…ØªØµÙØ­ Playwright.")


# --- Ø¯ÙˆØ§Ù„ ØªÙŠÙ„ÙŠØ¬Ø±Ø§Ù… (download_and_send_pdf) ---
async def download_and_send_pdf(context, chat_id, pdf_url, title="book.pdf"):
    """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„ÙØŒ Ø¥Ø±Ø³Ø§Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ØŒ Ø«Ù… Ø­Ø°ÙÙ‡ Ù…Ù† Ø§Ù„Ù‚Ø±Øµ Ø§Ù„ØµÙ„Ø¨."""
    tmp_dir = tempfile.gettempdir()
    file_path = os.path.join(tmp_dir, title.replace("/", "_")[:40] + ".pdf")
    
    async with ClientSession() as session:
        async with session.get(pdf_url, headers=USER_AGENT_HEADER) as resp:
            if resp.status != 200:
                await context.bot.send_message(
                    chat_id=chat_id, 
                    text=f"âš ï¸ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù Ù…Ù† Ø§Ù„Ù…ØµØ¯Ø±. Ø±Ù…Ø² Ø§Ù„Ø®Ø·Ø£: {resp.status}"
                )
                return
            
            content = await resp.read()

            if len(content) < MIN_PDF_SIZE_BYTES:
                await context.bot.send_message(
                    chat_id=chat_id, 
                    text="âš ï¸ ÙØ´Ù„ Ø§Ù„ØªØ­Ù…ÙŠÙ„: Ø­Ø¬Ù… Ø§Ù„Ù…Ù„Ù ØµØºÙŠØ± Ø¬Ø¯Ø§Ù‹ (ØºÙŠØ± ØµØ§Ù„Ø­). Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ø§Ù„Ø±Ø§Ø¨Ø· Ø®Ø§Ø·Ø¦Ø§Ù‹."
                )
                return
            
            async with aiofiles.open(file_path, "wb") as f:
                await f.write(content)
            
            try:
                with open(file_path, "rb") as f:
                    await context.bot.send_document(
                        chat_id=chat_id, 
                        document=f
                    )
                await context.bot.send_message(chat_id=chat_id, text="âœ… ØªÙ… Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„ÙƒØªØ§Ø¨ Ø¨Ù†Ø¬Ø§Ø­.")
            except Exception as e:
                 await context.bot.send_message(chat_id=chat_id, text=f"âš ï¸ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù…Ù„Ù Ø¥Ù„Ù‰ ØªÙŠÙ„ÙŠØ¬Ø±Ø§Ù…: {e}")
            finally:
                if os.path.exists(file_path):
                    os.remove(file_path)
                
# --- Ø¯ÙˆØ§Ù„ Ø£ÙˆØ§Ù…Ø± ØªÙŠÙ„ÙŠØ¬Ø±Ø§Ù… (Telegram Commands) ---

async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text(
        "ğŸ“š Ø¨ÙˆØª Ø§Ù„Ù‚ÙŠØ§Ù…Ø© Ø¬Ø§Ù‡Ø²!\n"
        "Ø£Ø±Ø³Ù„ /search Ù…ØªØ¨ÙˆØ¹Ù‹Ø§ Ø¨Ø§Ø³Ù… Ø§Ù„ÙƒØªØ§Ø¨ Ø£Ùˆ Ø§Ù„Ù…Ø¤Ù„Ù."
    )

async def search_cmd(update: Update, context: ContextTypes.DEFAULT_TYPE):
    query = " ".join(context.args).strip()
    if not query:
        await update.message.reply_text("Ø§Ø³ØªØ®Ø¯Ù…: /search Ø§Ø³Ù… Ø§Ù„ÙƒØªØ§Ø¨ Ø£Ùˆ Ø§Ù„Ù…Ø¤Ù„Ù")
        return

    # ğŸ’¥ Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø¯Ø§Ù„Ø© Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…Ø®ØµØµ (V9.0)
    msg = await update.message.reply_text(f"ğŸ” Ø£Ø¨Ø­Ø« Ø¹Ù† **{query}** (Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…Ø®ØµØµ Ø¯Ø§Ø®Ù„ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª)...")
    
    try:
        results = await search_site_and_extract_links(query) # ğŸ’¥ Ø§Ù„ØªØºÙŠÙŠØ± Ù‡Ù†Ø§

        if not results:
            await msg.edit_text("âŒ Ù„Ù… Ø£Ø¬Ø¯ Ù†ØªØ§Ø¦Ø¬ Ù…ÙˆØ«ÙˆÙ‚Ø© ÙÙŠ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù…Ø®ØªØ§Ø±Ø©. Ø­Ø§ÙˆÙ„ Ø¨ÙƒÙ„Ù…Ø§Øª Ù…Ø®ØªÙ„ÙØ©.")
            return

        buttons = []
        text_lines = ["**Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¨Ø­Ø«:**"]
        
        context.user_data[TEMP_LINKS_KEY] = [item.get("link") for item in results]
        
        for i, item in enumerate(results, start=0):
            title = item.get("title")[:120]
            # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…ØµØ¯Ø± Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø±Ø§Ø¨Ø·
            source = next((d.replace('.com', '').replace('.net', '').replace('.org', '') for d in TRUSTED_DOMAINS if d in item.get('link')), "Ù…ÙˆÙ‚Ø¹ Ø¢Ø®Ø±")
            
            text_lines.append(f"\n*{i+1}. {title}* (Ø§Ù„Ù…ØµØ¯Ø±: {source})")
            
            # Ø£Ø²Ø±Ø§Ø± Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© (V8.0)
            row1 = [
                InlineKeyboardButton(f"ğŸ“¥ ØªØ­Ù…ÙŠÙ„ {i+1}", callback_data=f"dl|{i}"),
                InlineKeyboardButton(f"ğŸ”— Ø±Ø§Ø¨Ø· Ø§Ù„Ù…ØµØ¯Ø±", url=item.get("link")) 
            ]
            buttons.append(row1)
        
        control_buttons = [
            InlineKeyboardButton("ğŸ” Ø¨Ø­Ø« Ø¬Ø¯ÙŠØ¯", switch_inline_query_current_chat="/search "),
            InlineKeyboardButton("âŒ Ø¥Ø®ÙØ§Ø¡ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø©", callback_data="hide")
        ]
        buttons.append(control_buttons)
        
        reply = "\n".join(text_lines)
        await msg.edit_text(reply, parse_mode='Markdown', reply_markup=InlineKeyboardMarkup(buttons))
        
    except Exception as e:
         await msg.edit_text(f"âš ï¸ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø¨Ø­Ø«: {e}")


async def callback_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    query = update.callback_query
    await query.answer()
    data = query.data
    
    # Ù…Ø¹Ø§Ù„Ø¬ Ø²Ø± Ø§Ù„Ø¥Ø®ÙØ§Ø¡ (V8.0)
    if data == "hide":
        try:
            await query.edit_message_text("âœ… ØªÙ… Ø¥Ø®ÙØ§Ø¡ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¨Ø­Ø«. Ø§Ø¨Ø¯Ø£ Ø¨Ø­Ø«Ù‹Ø§ Ø¬Ø¯ÙŠØ¯Ù‹Ø§ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… /search.")
        except:
             await context.bot.send_message(
                chat_id=query.message.chat_id,
                text="âœ… ØªÙ… Ø¥Ø®ÙØ§Ø¡ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¨Ø­Ø«. Ø§Ø¨Ø¯Ø£ Ø¨Ø­Ø«Ù‹Ø§ Ø¬Ø¯ÙŠØ¯Ù‹Ø§ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… /search.",
            )
        return

    if data.startswith("dl|"):
        try:
            index_str = data.split("|", 1)[1]
            index = int(index_str)
            link = context.user_data[TEMP_LINKS_KEY][index]

        except Exception:
            await context.bot.send_message(
                chat_id=query.message.chat_id,
                text="âš ï¸ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø²Ø± Ø§Ù„ØªØ­Ù…ÙŠÙ„ (Ø±Ø§Ø¨Ø· ØºÙŠØ± ØµØ§Ù„Ø­). ÙŠØ±Ø¬Ù‰ Ø§Ù„Ø¨Ø­Ø« Ù…Ø¬Ø¯Ø¯Ø§Ù‹.",
            )
            return
            
        await query.edit_message_text("â³ ØªÙØ¹ÙŠÙ„ Ø§Ù„ØªÙ†ØµØª Ø¹Ù„Ù‰ Ù†ÙˆØ¹ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ (MIME Type) Ù„Ø¹Ø¨ÙˆØ± Ø§Ù„Ø­Ù…Ø§ÙŠØ©...")
        
        try:
            pdf_link, title = await get_pdf_link_from_page(link)
            
            if pdf_link:
                await download_and_send_pdf(context, query.message.chat_id, pdf_link, title=title if title else "book")
            else:
                await context.bot.send_message(
                    chat_id=query.message.chat_id,
                    text=f"ğŸ“„ ÙØ´Ù„ Ø§Ù„Ø§Ø³ØªØ®Ù„Ø§Øµ. Ù‚Ø¯ ØªÙƒÙˆÙ† Ø§Ù„Ø­Ù…Ø§ÙŠØ© Ù‚ÙˆÙŠØ© Ø¬Ø¯Ø§Ù‹. Ø±Ø§Ø¨Ø· Ø§Ù„Ù…ØµØ¯Ø±: {link}",
                )
        
        except Exception as e:
            await context.bot.send_message(
                chat_id=query.message.chat_id,
                text=f"âš ï¸ Ø®Ø·Ø£ Playwright Ø£Ø«Ù†Ø§Ø¡ Ø¬Ù„Ø¨ Ø§Ù„Ù…Ù„Ù: {e}",
            )


def main():
    if not BOT_TOKEN:
        raise ValueError("BOT_TOKEN is missing in environment variables.")

    app = ApplicationBuilder().token(BOT_TOKEN).build()

    app.add_handler(CommandHandler("start", start))
    app.add_handler(CommandHandler("search", search_cmd))
    app.add_handler(CallbackQueryHandler(callback_handler))

    print("Ø§Ù„Ø¨ÙˆØª Ø¨Ø¯Ø£ Ø§Ù„Ø¹Ù…Ù„.")
    app.run_polling()

if __name__ == "__main__":
    main()
