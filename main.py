import os
import asyncio
import tempfile
import aiofiles
from aiohttp import ClientSession
from bs4 import BeautifulSoup
from telegram import InlineKeyboardButton, InlineKeyboardMarkup, Update
from telegram.ext import ApplicationBuilder, CommandHandler, CallbackQueryHandler, ContextTypes 
from playwright.async_api import async_playwright, Page 
from urllib.parse import urljoin 
from ddgs import DDGS 

# --- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø¨ÙˆØª ÙˆØ§Ù„Ø«ÙˆØ§Ø¨Øª ---
BOT_TOKEN = os.getenv("BOT_TOKEN")

# ÙˆÙƒÙŠÙ„ Ù…Ø³ØªØ®Ø¯Ù… Ù…Ø­Ù…ÙˆÙ„ (Mobile User Agent) Ù„Ø²ÙŠØ§Ø¯Ø© Ø§Ù„ØªØ­ØµÙŠÙ†
USER_AGENT = 'Mozilla/5.0 (iPhone; CPU iPhone OS 15_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.0 Mobile/15E148 Safari/604.1'
USER_AGENT_HEADER = {'User-Agent': USER_AGENT}

MIN_PDF_SIZE_BYTES = 50 * 1024 
TEMP_LINKS_KEY = "current_search_links" 
TRUSTED_DOMAINS = [
    "kotobati.com", 
    "masaha.org", 
    "books-library.net"
]

# --- Ø¯Ø§Ù„Ø© Ø§Ù„Ø¨Ø­Ø« (DuckDuckGo - Ø¨Ø¯ÙˆÙ† ØªØºÙŠÙŠØ±) ---
async def search_duckduckgo(query: str):
    """ÙŠØ³ØªØ®Ø¯Ù… DuckDuckGo API Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø±ÙˆØ§Ø¨Ø· PDF Ù…Ø¨Ø§Ø´Ø±Ø© ÙÙŠ Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„Ù…ÙˆØ«ÙˆÙ‚Ø©."""
    
    sites_query = " OR ".join([f"site:{d}" for d in TRUSTED_DOMAINS])
    full_query = f"{query} filetype:pdf OR {sites_query}"
    
    print(f"Executing search query: {full_query}")
    
    results = []
    
    try:
        with DDGS(timeout=5) as ddgs:
            search_results = ddgs.text(full_query, max_results=10)
            
            for r in search_results:
                link = r.get("href")
                title = r.get("title")
                
                if title and link and (any(d in link for d in TRUSTED_DOMAINS) or link.lower().endswith(".pdf")):
                    results.append({"title": title.strip(), "link": link})
    except Exception as e:
        print(f"DDGS search failed: {e}")
        return []

    unique_links = {}
    for item in results:
        unique_links[item['link']] = item
    
    return list(unique_links.values())[:5]


# --- Ø§Ù„Ø¥Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ø±Ø§Ø¨Ø¹Ø© Ø§Ù„Ù…Ø¨ØªÙƒØ±Ø©: Ø§Ù„ØªÙ†Ù‚ÙŠØ¨ ÙÙŠ Ø¬Ù…ÙŠØ¹ Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø´Ø¨ÙƒØ© ---
async def fallback_strategy_4_network_mine(page: Page, download_selector_css: str, link: str):
    
    network_urls = set()

    def capture_url(response):
        if response.status in [200, 206, 301, 302]:
            network_urls.add(response.url)
            
    page.on("response", capture_url)
    
    try:
        # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ù†Ù‚Ø± Ø£ÙˆÙ„Ø§Ù‹ Ù„ØªÙØ¹ÙŠÙ„ Ø´Ø¨ÙƒØ© Ø§Ù„ØªØ­Ù…ÙŠÙ„
        await page.locator(download_selector_css).click(timeout=10000) 
        await asyncio.sleep(7) 
        
        for url in network_urls:
            url_lower = url.lower()
            if url_lower.endswith('.pdf') or 'drive.google.com' in url_lower or 'dropbox.com' in url_lower or 'archive.org/download' in url_lower:
                print(f"PDF link found via Network Mining: {url}")
                return url
        
        return None 
        
    except Exception as e:
        print(f"Network mining click failed: {e}")
        return None
        
    finally:
        try:
            page.remove_listener("response", capture_url)
        except:
            pass 

# --- Ø¯Ø§Ù„Ø© Ø§Ù„Ø§Ø³ØªØ®Ù„Ø§Øµ Ø§Ù„Ù…Ø·Ù„Ù‚Ø© Ø§Ù„Ù…ÙØ·ÙˆÙ‘Ø±Ø© (V10.1 - Ø§Ù„Ø§Ø³ØªØ®Ù„Ø§Øµ Ø§Ù„Ù†Ø§Ø±ÙŠ) ---
async def get_pdf_link_from_page(link: str):
    """
    ØªØ³ØªØ®Ø¯Ù… Playwright Ø¨Ø®ÙŠØ§Ø±Ø§Øª ØªØ­ØµÙŠÙ† Ù…ØªÙ‚Ø¯Ù…Ø© (Ù…Ø­Ø§ÙƒØ§Ø© Ø¬Ù‡Ø§Ø² Ù…Ø­Ù…ÙˆÙ„) Ù„Ù…Ø­Ø§ÙƒØ§Ø© Ø§Ù„Ø¶ØºØ· 
    ÙˆØªØ·Ø¨ÙŠÙ‚ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø§Ù†ØªØ¸Ø§Ø± Ø°ÙƒÙŠØ© ÙˆØªÙ†Ù‚ÙŠØ¨ Ø´Ø¨ÙƒÙŠ Ø¹Ù…ÙŠÙ‚ Ù„Ø§Ø³ØªØ®Ù„Ø§Øµ Ø±Ø§Ø¨Ø· PDF.
    """
    pdf_link = None
    page_title = "book" 
    browser = None 
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ø£ÙˆÙ„
    if link.lower().endswith('.pdf') or 'archive.org/download' in link.lower() or 'drive.google.com' in link.lower():
        return link, "Direct PDF"
        
    try:
        async with async_playwright() as p:
            # ğŸ’¥ Ø§Ù„Ø§Ø¨ØªÙƒØ§Ø± 1: Ø¥Ø·Ù„Ø§Ù‚ Ø§Ù„Ù…ØªØµÙØ­ Ø¨Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„ØªØ­ØµÙŠÙ† Ø§Ù„Ù‚ØµÙˆÙ‰ ÙˆÙ…Ø­Ø§ÙƒØ§Ø© Ø¬Ù‡Ø§Ø² Ù…Ø­Ù…ÙˆÙ„
            iphone_13 = p.devices['iPhone 13']
            
            browser = await p.chromium.launch(
                headless=True,
                args=[
                    '--no-sandbox', 
                    '--disable-setuid-sandbox',
                    '--disable-blink-features=AutomationControlled', 
                    f'--user-agent={iphone_13["user_agent"]}' 
                ]
            )
            # ØªØ·Ø¨ÙŠÙ‚ Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„Ø¬Ù‡Ø§Ø² Ø§Ù„Ù…Ø­Ù…ÙˆÙ„
            context = await browser.new_context(**iphone_13) 
            page = await context.new_page()

            await page.goto(link, wait_until="domcontentloaded", timeout=40000) 
            
            # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ø£ÙˆÙ„ÙŠ
            html_content = await page.content()
            soup = BeautifulSoup(html_content, "html.parser")
            page_title = soup.title.string if soup.title else "book"
            
            # Ù…Ø­Ø¯Ø¯Ø§Øª Ø¹Ø§Ù…Ø© Ù„Ø£Ø²Ø±Ø§Ø± Ø§Ù„ØªØ­Ù…ÙŠÙ„
            download_selector_css = 'a[href*="pdf"], a.book-dl-btn, a.btn-download, button:has-text("ØªØ­Ù…ÙŠÙ„"), a:has-text("Download"), a:has-text("Ø§Ø¨Ø¯Ø£ Ø§Ù„ØªØ­Ù…ÙŠÙ„"), a:has-text("Ø§Ø¶ØºØ· Ù‡Ù†Ø§ Ù„Ù„ØªØ­Ù…ÙŠÙ„")'
            
            # --- Ø§Ù„Ø§Ø¨ØªÙƒØ§Ø± 2: Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø± Ø§Ù„Ø°ÙƒÙŠ Ù„Ø¸Ù‡ÙˆØ± Ø±Ø§Ø¨Ø· PDF ÙÙŠ Ø£ÙŠ Ù…ÙƒØ§Ù† ÙÙŠ Ø§Ù„ØµÙØ­Ø© ---
            try:
                # Ù†Ù†ØªØ¸Ø± Ø¸Ù‡ÙˆØ± Ø±Ø§Ø¨Ø· ÙŠÙ†ØªÙ‡ÙŠ Ø¨Ù€ .pdf Ø£Ùˆ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ 'download' Ø£Ùˆ google drive
                await page.wait_for_selector('a[href$=".pdf"], a[href*="download"], a[href*="drive.google.com"]', timeout=10000)
                
                # Ø¥Ø°Ø§ Ø¸Ù‡Ø±ØŒ Ù†ÙƒØªØ´ÙÙ‡ Ù…Ù† HTML Ù…Ø¨Ø§Ø´Ø±Ø©
                html_content = await page.content()
                soup = BeautifulSoup(html_content, "html.parser")
                
                for a_tag in soup.find_all('a', href=True):
                    href = urljoin(link, a_tag['href'])
                    if href.lower().endswith('.pdf') or 'download' in href.lower() or 'drive.google.com' in href.lower():
                        pdf_link = href
                        print(f"PDF link found via Smart Wait: {pdf_link}")
                        break
                        
            except Exception:
                pass 
                
            if pdf_link:
                return pdf_link, page_title

            
            # --- Ù…Ø­Ø§ÙˆÙ„Ø© 1: Ø§Ù„ØªØ²Ø§Ù…Ù† (gather) ---
            try:
                pdf_response, _ = await asyncio.gather(
                    page.wait_for_response(
                        lambda response: response.status in [200, 206, 301, 302] and (
                            'application/pdf' in response.headers.get('content-type', '') or 
                            response.url.lower().endswith('.pdf')
                        ),
                        timeout=30000
                    ),
                    page.click(download_selector_css, timeout=25000) 
                )
                pdf_link = pdf_response.url
                
            except Exception as e:
                print(f"Initial gather failed, attempting fallback strategies: {e}")
                
                # --- Ù…Ø­Ø§ÙˆÙ„Ø© 2: Ø§Ù„ØªÙ†Ù‚ÙŠØ¨ Ø§Ù„Ø´Ø¨ÙƒÙŠ Ø§Ù„Ø¹Ù…ÙŠÙ‚ (Ø§Ù„Ø§Ø¨ØªÙƒØ§Ø± 3) ---
                print("Executing Deep Network Mining (Strategy 4 - Early Attempt).")
                pdf_link = await fallback_strategy_4_network_mine(page, download_selector_css, link)
                
                if not pdf_link:
                    # Ø¥Ø°Ø§ ÙØ´Ù„ Ø§Ù„ØªÙ†Ù‚ÙŠØ¨ Ø§Ù„Ù…Ø¨ÙƒØ±ØŒ Ù†Ø¹ÙˆØ¯ Ù„ØªØ­Ù„ÙŠÙ„ HTML Ø§Ù„Ù‚Ø¯ÙŠÙ… ÙƒØ®ÙŠØ§Ø± Ø£Ø®ÙŠØ±
                    await asyncio.sleep(5) 
                    final_html_content = await page.content()
                    final_soup = BeautifulSoup(final_html_content, "html.parser")
                    
                    for a_tag in final_soup.find_all('a', href=True):
                        href = urljoin(link, a_tag['href'])
                        href_lower = href.lower()
                        
                        if href_lower.endswith('.pdf') or 'download' in href_lower:
                            pdf_link = href
                            print(f"General link found in HTML (Strategy 3 - Final): {pdf_link}")
                            break

            # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
            if not page_title:
                 html_content = await page.content()
                 soup = BeautifulSoup(html_content, "html.parser")
                 page_title = soup.title.string if soup.title else "book"

            return pdf_link, page_title
    
    except Exception as e:
        print(f"Critical error in get_pdf_link_from_page: {e}")
        raise e
    
    finally:
        if browser:
            await browser.close()


# --- Ø¨Ø§Ù‚ÙŠ Ø¯ÙˆØ§Ù„ ØªÙŠÙ„ÙŠØ¬Ø±Ø§Ù… (download_and_send_pdfØŒ startØŒ search_cmdØŒ callback_handlerØŒ main) ---
async def download_and_send_pdf(context, chat_id, pdf_url, title="book.pdf"):
    """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„ÙØŒ Ø¥Ø±Ø³Ø§Ù„Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ØŒ Ø«Ù… Ø­Ø°ÙÙ‡ Ù…Ù† Ø§Ù„Ù‚Ø±Øµ Ø§Ù„ØµÙ„Ø¨."""
    tmp_dir = tempfile.gettempdir()
    file_path = os.path.join(tmp_dir, title.replace("/", "_")[:40] + ".pdf")
    
    async with ClientSession() as session:
        async with session.get(pdf_url, headers=USER_AGENT_HEADER) as resp:
            if resp.status != 200:
                await context.bot.send_message(
                    chat_id=chat_id, 
                    text=f"âš ï¸ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù Ù…Ù† Ø§Ù„Ù…ØµØ¯Ø±. Ø±Ù…Ø² Ø§Ù„Ø®Ø·Ø£: {resp.status}"
                )
                return
            
            content = await resp.read()

            if len(content) < MIN_PDF_SIZE_BYTES:
                await context.bot.send_message(
                    chat_id=chat_id, 
                    text="âš ï¸ ÙØ´Ù„ Ø§Ù„ØªØ­Ù…ÙŠÙ„: Ø­Ø¬Ù… Ø§Ù„Ù…Ù„Ù ØµØºÙŠØ± Ø¬Ø¯Ø§Ù‹ (ØºÙŠØ± ØµØ§Ù„Ø­). Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ø§Ù„Ø±Ø§Ø¨Ø· Ø®Ø§Ø·Ø¦Ø§Ù‹."
                )
                return
            
            async with aiofiles.open(file_path, "wb") as f:
                await f.write(content)
            
            try:
                with open(file_path, "rb") as f:
                    await context.bot.send_document(
                        chat_id=chat_id, 
                        document=f
                    )
                await context.bot.send_message(chat_id=chat_id, text="âœ… ØªÙ… Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„ÙƒØªØ§Ø¨ Ø¨Ù†Ø¬Ø§Ø­.")
            except Exception as e:
                 await context.bot.send_message(chat_id=chat_id, text=f"âš ï¸ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù…Ù„Ù Ø¥Ù„Ù‰ ØªÙŠÙ„ÙŠØ¬Ø±Ø§Ù…: {e}")
            finally:
                if os.path.exists(file_path):
                    os.remove(file_path)
                
# --- Ø¯ÙˆØ§Ù„ Ø£ÙˆØ§Ù…Ø± ØªÙŠÙ„ÙŠØ¬Ø±Ø§Ù… (Telegram Commands) ---

async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text(
        "ğŸ“š Ø¨ÙˆØª Ø§Ù„Ù‚ÙŠØ§Ù…Ø© Ø¬Ø§Ù‡Ø²!\n"
        "Ø£Ø±Ø³Ù„ /search Ù…ØªØ¨ÙˆØ¹Ù‹Ø§ Ø¨Ø§Ø³Ù… Ø§Ù„ÙƒØªØ§Ø¨ Ø£Ùˆ Ø§Ù„Ù…Ø¤Ù„Ù."
    )

async def search_cmd(update: Update, context: ContextTypes.DEFAULT_TYPE):
    query = " ".join(context.args).strip()
    if not query:
        await update.message.reply_text("Ø§Ø³ØªØ®Ø¯Ù…: /search Ø§Ø³Ù… Ø§Ù„ÙƒØªØ§Ø¨ Ø£Ùˆ Ø§Ù„Ù…Ø¤Ù„Ù")
        return

    msg = await update.message.reply_text(f"ğŸ” Ø£Ø¨Ø­Ø« Ø¹Ù† **{query}** Ø¹Ø¨Ø± **DuckDuckGo** (ØºÙŠØ± Ù…Ù‚ÙŠØ¯)...")
    
    try:
        results = await search_duckduckgo(query)

        if not results:
            await msg.edit_text("âŒ Ù„Ù… Ø£Ø¬Ø¯ Ù†ØªØ§Ø¦Ø¬ Ù…ÙˆØ«ÙˆÙ‚Ø© ÙÙŠ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù…Ø®ØªØ§Ø±Ø©. Ø­Ø§ÙˆÙ„ Ø¨ÙƒÙ„Ù…Ø§Øª Ù…Ø®ØªÙ„ÙØ© Ø£Ùˆ Ø¬Ø±Ø¨ Ø§Ù„Ø¨Ø­Ø« Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.")
            return

        buttons = []
        text_lines = []
        
        context.user_data[TEMP_LINKS_KEY] = [item.get("link") for item in results]
        
        for i, item in enumerate(results, start=0):
            title = item.get("title")[:120]
            source = next((d.replace('.com', '').replace('.net', '') for d in TRUSTED_DOMAINS if d in item.get('link')), "Ù…Ø¨Ø§Ø´Ø±/Ø¹Ø§Ù…")
            text_lines.append(f"{i+1}. {title} (Ø§Ù„Ù…ØµØ¯Ø±: {source})")
            buttons.append([InlineKeyboardButton(f"ğŸ“¥ ØªØ­Ù…ÙŠÙ„ {i+1}", callback_data=f"dl|{i}")])
            
        reply = "\n".join(text_lines)
        await msg.edit_text(reply, reply_markup=InlineKeyboardMarkup(buttons))
        
    except Exception as e:
         await msg.edit_text(f"âš ï¸ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø¨Ø­Ø«: {e}")


async def callback_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    query = update.callback_query
    await query.answer()
    data = query.data
    
    if data.startswith("dl|"):
        try:
            index_str = data.split("|", 1)[1]
            index = int(index_str)
            link = context.user_data[TEMP_LINKS_KEY][index]

        except Exception:
            await context.bot.send_message(
                chat_id=query.message.chat_id,
                text="âš ï¸ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø²Ø± Ø§Ù„ØªØ­Ù…ÙŠÙ„ (Ø±Ø§Ø¨Ø· ØºÙŠØ± ØµØ§Ù„Ø­). ÙŠØ±Ø¬Ù‰ Ø§Ù„Ø¨Ø­Ø« Ù…Ø¬Ø¯Ø¯Ø§Ù‹.",
            )
            return
            
        await query.edit_message_text("â³ ØªÙØ¹ÙŠÙ„ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ø§Ø³ØªØ®Ù„Ø§Øµ Ø§Ù„Ù†Ø§Ø±ÙŠ (V10.1)...")
        
        try:
            pdf_link, title = await get_pdf_link_from_page(link)
            
            if pdf_link:
                await download_and_send_pdf(context, query.message.chat_id, pdf_link, title=title if title else "book")
            else:
                await context.bot.send_message(
                    chat_id=query.message.chat_id,
                    text=f"ğŸ“„ ÙØ´Ù„ Ø§Ù„Ø§Ø³ØªØ®Ù„Ø§Øµ. Ù‚Ø¯ ØªÙƒÙˆÙ† Ø§Ù„Ø­Ù…Ø§ÙŠØ© Ù‚ÙˆÙŠØ© Ø¬Ø¯Ø§Ù‹. Ø±Ø§Ø¨Ø· Ø§Ù„Ù…ØµØ¯Ø±: {link}",
                )
        
        except Exception as e:
            await context.bot.send_message(
                chat_id=query.message.chat_id,
                text=f"âš ï¸ Ø®Ø·Ø£ Playwright Ø£Ø«Ù†Ø§Ø¡ Ø¬Ù„Ø¨ Ø§Ù„Ù…Ù„Ù: {e}",
            )


def main():
    if not BOT_TOKEN:
        raise ValueError("BOT_TOKEN is missing in environment variables.")

    app = ApplicationBuilder().token(BOT_TOKEN).build()

    app.add_handler(CommandHandler("start", start))
    app.add_handler(CommandHandler("search", search_cmd))
    app.add_handler(CallbackQueryHandler(callback_handler))

    print("Ø§Ù„Ø¨ÙˆØª Ø¨Ø¯Ø£ Ø§Ù„Ø¹Ù…Ù„.")
    app.run_polling()

if __name__ == "__main__":
    main()
