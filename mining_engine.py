import joblib
import pandas as pd
import numpy as np
import asyncio
import re
from typing import List, Dict, Any, Optional
# Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù‡Ø§Ù…Ø© Ù„Ù„Ø¨ÙŠØ¦Ø© (ØªØ°ÙƒØ± Ø¥Ø¶Ø§ÙØªÙ‡Ø§ ÙÙŠ requirements.txt)
from bs4 import BeautifulSoup 
from playwright.async_api import async_playwright 

# -----------------------------------------------------
#                ÙˆØ­Ø¯Ø© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ (AI Selector)
# -----------------------------------------------------

# Ø¯Ø§Ù„Ø© Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ù…ÙŠØ²Ø§Øª Ù„Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ (Inference) - 8 Ù…ÙŠØ²Ø§Øª
def feature_engineer_for_inference(record: dict) -> list:
    """ØªØ³ØªØ®Ø±Ø¬ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø«Ù…Ø§Ù†ÙŠØ© Ø¨Ù†ÙØ³ Ø§Ù„ØªØ±ØªÙŠØ¨ Ø§Ù„Ø°ÙŠ ØªÙ… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¹Ù„ÙŠÙ‡."""
    
    text_content = record.get('text_content', '')
    tag_type = record.get('tag_type', '')
    css_class = record.get('css_class', '')
    href = record.get('href', '')
    css_selector = record.get('css_selector', '')
    is_near_pdf_keyword = record.get('is_near_pdf_keyword', 0)
    feat_depth = record.get('feat_depth', 0)
    feat_is_in_main_section = record.get('feat_is_in_main_section', 0)

    features = []
    
    # Ø§Ù„Ù€ 8 Ù…ÙŠØ²Ø§Øª Ø¨Ø§Ù„ØªØ±ØªÙŠØ¨:
    features.append(1.0 if ('ØªØ­Ù…ÙŠÙ„' in text_content or 'download' in text_content.lower()) else 0.0)
    features.append(1.0 if tag_type == 'a' else 0.0)
    features.append(float(len(css_class.split()) if css_class else 0.0))
    features.append(float(is_near_pdf_keyword))
    features.append(1.0 if (href and (href.endswith('.pdf') or href.endswith('.zip') or href.endswith('.epub'))) else 0.0)
    features.append(float(css_selector.count('.') + css_selector.count('#') if css_selector else 0.0))
    features.append(float(feat_depth))
    features.append(float(feat_is_in_main_section))
    
    return features


# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙŠØªÙ… Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø©)
try:
    AI_SELECTOR_MODEL = joblib.load('selector_classifier_model.pkl')
    print("âœ… ÙˆØ­Ø¯Ø© MiningEngine: ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¨Ù†Ø¬Ø§Ø­.")
except Exception as e:
    AI_SELECTOR_MODEL = None
    print(f"âŒ ÙˆØ­Ø¯Ø© MiningEngine: ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ. Ø§Ù„Ø®Ø·Ø£: {e}")

# -----------------------------------------------------
#                   ÙƒÙ„Ø§Ø³ MiningEngine
# -----------------------------------------------------

class MiningEngine:
    
    @staticmethod
    async def get_pdf_link_and_headers(page: Any) -> Optional[Dict[str, Any]]:
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        if AI_SELECTOR_MODEL is None:
            return None 

        html_content = await page.content()
        soup = BeautifulSoup(html_content, 'html.parser')
        
        best_selector = None
        max_probability = 0.0
        candidates = []

        # 1. Ø¬Ù…Ø¹ Ø§Ù„Ù…Ø±Ø´Ø­ÙŠÙ† ÙˆØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù‡ÙŠÙƒÙ„ÙŠØ©
        for tag in soup.find_all(['a', 'button']):
            href = tag.get('href')
            if not href or href.startswith('#'):
                continue
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¹Ù…Ù‚ ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù‚Ø³Ù… Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
            parent_count = 0
            current_tag = tag
            while current_tag.parent is not None and current_tag.parent.name not in ['[document]', 'html']:
                parent_count += 1
                current_tag = current_tag.parent
            is_in_main = 1 if tag.find_parent(['main', 'article']) else 0
            
            record = {
                "text_content": tag.get_text().strip(),
                "tag_type": tag.name,
                "css_class": tag.get('class', [''])[0],
                # Ù…ÙØ­Ø¯Ù‘ÙØ¯ Ø¨Ø³ÙŠØ· Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙÙŠ Ø§Ù„Ù†Ù‚Ø±
                "css_selector": f"{tag.name}[href='{href}']", 
                "href": href,
                "feat_depth": parent_count,
                "feat_is_in_main_section": is_in_main,
                "is_near_pdf_keyword": 1 if 'pdf' in tag.get_text().lower() else 0
            }
            candidates.append(record)

        if not candidates:
            return None

        # 2. ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù…Ø±Ø´Ø­ÙŠÙ† Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ
        for record in candidates:
            features = feature_engineer_for_inference(record)
            probability = AI_SELECTOR_MODEL.predict_proba(np.array([features]))[0][1] 
            
            if probability > max_probability:
                max_probability = probability
                best_selector = record['css_selector']
                best_href = record['href'] # Ø­ÙØ¸ Ø§Ù„Ø±Ø§Ø¨Ø· Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø§Ø­Ù‚Ø§Ù‹
        
        CONFIDENCE_THRESHOLD = 0.70 
        
        # 3. Ø§Ù„Ù‚Ø±Ø§Ø± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ ÙˆØ§Ù„Ù†Ù‚Ø±
        if max_probability < CONFIDENCE_THRESHOLD:
            return None
        
        print(f"âœ… ØªÙ… Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù…Ø­Ø¯Ø¯: {best_selector} ({max_probability:.4f})")
        
        # ğŸš¨ Ù…Ù†Ø·Ù‚ Ø§Ù„Ù†Ù‚Ø± Ø§Ù„ÙØ¹Ù„ÙŠ (ØªÙ… Ø¥ÙƒÙ…Ø§Ù„Ù‡ Ø¨Ù…Ù†Ø·Ù‚ Ù‚ÙŠØ§Ø³ÙŠ Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„ØªØ­Ù…ÙŠÙ„)
        
        # ØªØ¹Ø±ÙŠÙ Ù…ØªØºÙŠØ± Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© Ø±Ø§Ø¨Ø· Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
        download_url = None
        
        # Ø¯Ø§Ù„Ø© Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø´Ø¨ÙƒØ© ÙˆØ§Ù„ØªÙ‚Ø§Ø· Ø±Ø§Ø¨Ø· Ø§Ù„Ù…Ù„Ù
        def handle_download(download):
            nonlocal download_url
            download_url = download.url
            print(f"ğŸ“¥ ØªÙ… Ø§Ù„ØªÙ‚Ø§Ø· Ø±Ø§Ø¨Ø· Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø¨Ø§Ø´Ø±: {download_url}")
            
        page.on("download", handle_download)
        
        print(f"ğŸ–±ï¸ Ø§Ù„Ù†Ù‚Ø± Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø­Ø¯Ø¯: {best_selector}")
        await page.click(best_selector, timeout=15000)
        
        # Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø± Ø§Ù„Ù‚ØµÙŠØ± Ù„Ø¥ØªÙ…Ø§Ù… Ø§Ù„ØªØ­Ù…ÙŠÙ„
        await asyncio.sleep(2) 

        return {
            "selector": best_selector, 
            "confidence": max_probability,
            "final_download_link": download_url if download_url else best_href
        }


# -----------------------------------------------------
#                   Ù…Ù†Ø·Ù‚ Ø§Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ (Ù„Ù„ØªØ¬Ø±Ø¨Ø©)
# -----------------------------------------------------

async def run_mining_task(url: str):
    """Ø¯Ø§Ù„Ø© Ù„ÙØªØ­ Ø§Ù„Ù…ØªØµÙØ­ ÙˆØªÙ†ÙÙŠØ° Ù…Ù‡Ù…Ø© Ø§Ù„Ø§Ø³ØªØ®Ù„Ø§Øµ."""
    # (Ù‡Ø°Ø§ Ø§Ù„Ø¬Ø²Ø¡ Ù„Ù† ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù…Ø¨Ø§Ø´Ø±Ø© Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„Ø¨ÙˆØªØŒ ÙˆÙ„ÙƒÙ†Ù‡ Ù…ÙÙŠØ¯ Ù„Ù„ØªØ¬Ø±Ø¨Ø©)
    print(f"\n--- Ø¨Ø¯Ø¡ Ù…Ù‡Ù…Ø© Ø§Ù„Ø§Ø³ØªØ®Ù„Ø§Øµ Ù„Ù„Ø±Ø§Ø¨Ø·: {url} ---")
    
    async with async_playwright() as p:
        browser = await p.chromium.launch() 
        page = await browser.new_page()
        
        try:
            await page.goto(url, timeout=60000)
        except Exception:
            await browser.close()
            return

        result = await MiningEngine.get_pdf_link_and_headers(page)
        await browser.close()
        
        return result

# if __name__ == "__main__":
#     # ÙŠÙ…ÙƒÙ†Ùƒ ÙˆØ¶Ø¹ Ø±Ø§Ø¨Ø· Ø§Ø®ØªØ¨Ø§Ø± Ù‡Ù†Ø§
#     TEST_URL = "https://www.kotobati.com" 
#     asyncio.run(run_mining_task(TEST_URL))
