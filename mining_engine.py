import joblib
import pandas as pd
import numpy as np
# ... (Ø¨Ù‚ÙŠØ© Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯Ø§Øª: asyncio, playwright, aiohttp, re, BeautifulSoup, List, Dict, Optional) ...

# -----------------------------------------------------
#                ÙˆØ­Ø¯Ø© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ (AI Selector)
# -----------------------------------------------------

# ðŸš¨ Ø§Ù„Ø¥Ø¶Ø§ÙØ© 1: Ø¯Ø§Ù„Ø© Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ù…ÙŠØ²Ø§Øª Ù„Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ (Inference)
def feature_engineer_for_inference(record: dict) -> list:
    """ØªØ³ØªØ®Ø±Ø¬ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø«Ù…Ø§Ù†ÙŠØ© Ø¨Ù†ÙØ³ Ø§Ù„ØªØ±ØªÙŠØ¨ Ø§Ù„Ø°ÙŠ ØªÙ… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¹Ù„ÙŠÙ‡."""
    
    # Ø¶Ù…Ø§Ù† Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©
    text_content = record.get('text_content', '')
    tag_type = record.get('tag_type', '')
    css_class = record.get('css_class', '')
    href = record.get('href', '')
    css_selector = record.get('css_selector', '')
    is_near_pdf_keyword = record.get('is_near_pdf_keyword', 0)
    feat_depth = record.get('feat_depth', 0)
    feat_is_in_main_section = record.get('feat_is_in_main_section', 0)

    features = []
    
    # 1. feat_has_download_word
    features.append(1.0 if ('ØªØ­Ù…ÙŠÙ„' in text_content or 'download' in text_content.lower()) else 0.0)
    
    # 2. feat_is_anchor
    features.append(1.0 if tag_type == 'a' else 0.0)
    
    # 3. feat_class_length
    features.append(float(len(css_class.split()) if css_class else 0.0))
    
    # 4. feat_structural_proximity
    features.append(float(is_near_pdf_keyword))
    
    # 5. feat_is_file_link
    features.append(1.0 if (href and (href.endswith('.pdf') or href.endswith('.zip') or href.endswith('.epub'))) else 0.0)
    
    # 6. feat_selector_complexity
    features.append(float(css_selector.count('.') + css_selector.count('#') if css_selector else 0.0))
    
    # 7. feat_depth_v2
    features.append(float(feat_depth))
    
    # 8. feat_is_in_main_section_v2
    features.append(float(feat_is_in_main_section))
    
    return features


# ðŸš¨ Ø§Ù„Ø¥Ø¶Ø§ÙØ© 2: ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
try:
    AI_SELECTOR_MODEL = joblib.load('selector_classifier_model.pkl')
    print("âœ… ÙˆØ­Ø¯Ø© MiningEngine: ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¨Ù†Ø¬Ø§Ø­.")
except Exception as e:
    AI_SELECTOR_MODEL = None
    print(f"âŒ ÙˆØ­Ø¯Ø© MiningEngine: ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ. Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ù†Ø·Ù‚ Ø§Ù„ÙŠØ¯ÙˆÙŠ (Ø¥Ø°Ø§ ÙˆØ¬Ø¯). Ø§Ù„Ø®Ø·Ø£: {e}")

# -----------------------------------------------------
#                   ÙƒÙ„Ø§Ø³ MiningEngine
# -----------------------------------------------------

class MiningEngine:
    # ... (Ø¨Ù‚ÙŠØ© Ø§Ù„ÙƒÙˆØ¯) ...
    
    @staticmethod
    async def get_pdf_link_and_headers(page: Any) -> Optional[Dict[str, Any]]:
        # âš ï¸ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø£ÙˆÙ„Ø§Ù‹
        if AI_SELECTOR_MODEL is None:
            # ÙŠÙ…ÙƒÙ† ÙˆØ¶Ø¹ Ù…Ù†Ø·Ù‚ Ø¨Ø¯ÙŠÙ„ Ø£Ùˆ Ø¥Ø±Ø¬Ø§Ø¹ None Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù…Ø·Ù„ÙˆØ¨Ø§Ù‹
            return None 

        html_content = await page.content()
        soup = BeautifulSoup(html_content, 'html.parser')
        
        best_selector = None
        max_probability = 0.0
        
        candidates = []

        # 1. Ø¬Ù…Ø¹ Ø§Ù„Ù…Ø±Ø´Ø­ÙŠÙ† ÙˆØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù‡ÙŠÙƒÙ„ÙŠØ© ÙÙŠ ÙˆÙ‚Øª Ø§Ù„ØªØ´ØºÙŠÙ„
        for tag in soup.find_all(['a', 'button']):
            href = tag.get('href')
            if not href or href.startswith('#'):
                continue

            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¹Ù…Ù‚ (feat_depth)
            parent_count = 0
            current_tag = tag
            while current_tag.parent is not None and current_tag.parent.name not in ['[document]', 'html']:
                parent_count += 1
                current_tag = current_tag.parent
            
            # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù‚Ø³Ù… Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ (feat_is_in_main_section)
            is_in_main = 1 if tag.find_parent(['main', 'article']) else 0
            
            record = {
                "text_content": tag.get_text().strip(),
                "tag_type": tag.name,
                "css_class": tag.get('class', [''])[0],
                "css_selector": f"{tag.name}[href='{href}']", 
                "href": href,
                "feat_depth": parent_count,
                "feat_is_in_main_section": is_in_main,
                "is_near_pdf_keyword": 1 if 'pdf' in tag.get_text().lower() else 0
            }
            candidates.append(record)

        if not candidates:
            return None

        # 2. ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù…Ø±Ø´Ø­ÙŠÙ† Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ
        print("ðŸ§  ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù…Ø±Ø´Ø­ÙŠÙ† Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ...")
        
        for record in candidates:
            features = feature_engineer_for_inference(record)
            
            # Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ©
            # ÙŠØ¬Ø¨ Ø§Ø³ØªØ®Ø¯Ø§Ù… np.array Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ Ù…Ù‚Ø¨ÙˆÙ„ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬
            probability = AI_SELECTOR_MODEL.predict_proba(np.array([features]))[0][1] 
            
            if probability > max_probability:
                max_probability = probability
                best_selector = record['css_selector']
        
        CONFIDENCE_THRESHOLD = 0.70 
        
        if max_probability < CONFIDENCE_THRESHOLD:
            print(f"âš ï¸ ØªÙ†Ø¨ÙŠÙ‡: Ø£ÙØ¶Ù„ Ø§Ø­ØªÙ…Ø§Ù„ ({max_probability:.4f}) Ø£Ù‚Ù„ Ù…Ù† 70%. Ø³ÙŠØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ø§Ø³ØªØ®Ù„Ø§Øµ.")
            return None
        
        print(f"âœ… ØªÙ… Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù…Ø­Ø¯Ø¯ Ø¨Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ©: {best_selector} ({max_probability:.4f})")
        
        # ... (Ø¨Ù‚ÙŠØ© Ù…Ù†Ø·Ù‚ Ø§Ù„Ù†Ù‚Ø± ÙˆÙ…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø´Ø¨ÙƒØ©) ...
        return {"selector": best_selector, "confidence": max_probability}

# ... (Ø¨Ù‚ÙŠØ© Ø§Ù„ÙƒÙ„Ø§Ø³Ø§Øª ÙˆØ§Ù„Ø¯ÙˆØ§Ù„) ...
